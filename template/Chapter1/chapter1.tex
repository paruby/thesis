%!TEX root = ../thesis.tex
%*******************************************************************************
%*********************************** First Chapter *****************************
%*******************************************************************************

\chapter{Introduction}  %Title of the First Chapter

\ifpdf
    \graphicspath{{Chapter1/Figs/Raster/}{Chapter1/Figs/PDF/}{Chapter1/Figs/}}
\else
    \graphicspath{{Chapter1/Figs/Vector/}{Chapter1/Figs/}}
\fi


\nomenclature[A]{LVM}{Latent Variable Model}
\nomenclature[A]{ICA}{Independent Component Analysis}
\nomenclature[A]{GAN}{Generative Adversarial Network}
\nomenclature[A]{WAE}{Wasserstein Autoencoder}
\nomenclature[A]{VAE}{Variational Autoencoder}
\nomenclature[A]{\iid}{Independent and identically distributed}
\nomenclature[A]{IPM}{Integral Probability Metric}
\nomenclature[A]{MMD}{Maximum Mean Discrepancy}
\nomenclature[A]{RKHS}{Reproducing Kernel Hilbert Space}
\nomenclature[A]{MC}{Monte-Carlo}
\nomenclature[A]{RAM}{Random Mixture estimator}
\nomenclature[A]{RAM-MC}{Random Mixture estimator with Monte-Carlo sampling}
\nomenclature[A]{TC}{Total Correlation}
\nomenclature[A]{MWS}{Minibatch Weighted Sample}
\nomenclature[A]{MI}{Mutual Information}
\nomenclature[A]{ELBO}{Evidence Lower Bound}
\nomenclature[A]{CCA}{Canonical Correlation Analysis}
\nomenclature[A]{SDV}{Sufficiently Distinct Views}
\nomenclature[A]{SEM}{Structural Equation Model}
\nomenclature[A]{HDL}{High-Density Lipoprotein}
\nomenclature[A]{LDL}{Low-Density Lipoprotein}
\nomenclature[A]{HD}{Heart Disease}


\nomenclature[F]{KL}{Kullback-Leibler}
\nomenclature[F]{TV}{Total Variation}
\nomenclature[F]{$\Hsq$}{Squared-Hellinger}
\nomenclature[F]{JS}{Jensen-Shannon}



\nomenclature[M]{$A$}{Matrix}
\nomenclature[M]{$\mathcal{X}$}{Data space}
\nomenclature[M]{$\mathcal{Z}$}{Latent space}
\nomenclature[M]{$X$}{Random variable over data space}
\nomenclature[M]{$x$}{Particular value taken by $X$}
\nomenclature[M]{$\mathbf{X}^N$}{Set of $N$ observations of $X$}
\nomenclature[M]{$Z$}{Random variable over latent space}
\nomenclature[M]{$\mathbf{Z}^M$}{Set of $M$ observations of $Z$}
\nomenclature[M]{$E$}{Exogenous noise variable}
\nomenclature[M]{$N$}{Noise variable}
\nomenclature[M]{$Q_X$, $q(x)$}{Data distribution, density}
\nomenclature[M]{$P^\theta_X$, $p^\theta(x)$}{Model distribution with parameter $\theta$, density}
\nomenclature[M]{$P_Z$, $p(z)$}{Prior distribution over latent space, density}
\nomenclature[M]{$Q_Z$, $q(z)$}{Aggregate posterior distribution over latent space, density}
\nomenclature[M]{$\hat{Q}_Z^N$, $\hat{q}_N(z)$}{Empirical approximation to aggregate posterior based on $N$ samples, density}
\nomenclature[M]{$H(Q_Z)$}{Differential entropy of $Q_Z$}
\nomenclature[M]{$TC(Q_Z)$}{Total correlation of $Q_Z$}
\nomenclature[M]{$I(X,Z)$}{Mutual information of variables $X$ and $Z$}

\nomenclature[M]{$\mathcal{H}$}{Function class}
\nomenclature[M]{$L(\theta, \phi)$}{Loss function with parameters $\theta$ and $\phi$}
\nomenclature[M]{$OT_c$}{Optimal transport distance with respect to metric $c$}
\nomenclature[M]{$D, D_f$}{Divergence}
\nomenclature[M]{$\mathcal{N}(\mu, \Sigma)$}{Gaussian distribution with mean $\mu$ and covariance matrix $\Sigma$}
\nomenclature[M]{$\mathbb{E}$}{Expectation operator}
\nomenclature[M]{$\mathbb{P}$}{Probability operator}
\nomenclature[M]{$\#$}{Push-forward operator}
\nomenclature[M]{$\doop(X_i=x_i)$}{Do-intervention setting variable $X_i$ to value $x_i$}
\nomenclature[M]{$\mathcal{M}_X$}{Structural Equation Model over $X$}
\nomenclature[M]{$\mathcal{S}_X$}{Structural equations in $\mathcal{M}_X$}
\nomenclature[M]{$\mathcal{I}_X$}{Interventions in $\mathcal{M}_X$}
\nomenclature[M]{$P_X^{\doop(\cdot)}$}{Distribution over $X$ after intervention $\doop(\cdot)$}
\nomenclature[M]{$\mathcal{G}$}{Causal graph}
\nomenclature[M]{$\tau$}{Function mapping between causal variables}
\nomenclature[M]{$\omega$}{Function mapping between sets of interventions}
\nomenclature[M]{$\mathcal{P}(\mathcal{X})$}{The set of probability distributions over $\mathcal{X}$}

% Example nomenclature definitions

%\nomenclature[z-cif]{$CIF$}{Cauchy's Integral Formula}                                % first letter Z is for Acronyms 
%
%\nomenclature
%
%\nomenclature[a-F]{$F$}{complex function}                                                   % first letter A is for Roman symbols
%\nomenclature[g-p]{$\pi$}{ $\simeq 3.14\ldots$}                                             % first letter G is for Greek Symbols
%\nomenclature[g-i]{$\iota$}{unit imaginary number $\sqrt{-1}$}                      % first letter G is for Greek Symbols
%\nomenclature[g-g]{$\gamma$}{a simply closed curve on a complex plane}  % first letter G is for Greek Symbols
%\nomenclature[x-i]{$\oint_\gamma$}{integration around a curve $\gamma$} % first letter X is for Other Symbols
%\nomenclature[r-j]{$j$}{superscript index}                                                       % first letter R is for superscripts
%\nomenclature[s-0]{$0$}{subscript index}                                                        % first letter S is for subscripts

%\section{Non-technical introduction}
%
%Non-technical introduction that should be accessible to people who don't know about machine learning, e.g. my parents.
%\begin{itemize}
%\item Describe machine learning as way to program computers.
%\item When humans look at a picture, we don't see pixels. We immediately see higher level concepts.
%\item An important topic in ML, and the subject of this thesis, is, roughly speaking, how machines can learn high level concepts. 
%\item The example of images is easy to grasp because we are familiar with the idea of objects like cats and dogs. In fact, the difficult thing to understand is that images (on a screen) are fundamentally an array of numbers. 
%\item Another example: audio. If I showed you a picture of a wave form, you wouldn't understand what it is. But if I play it to you, you'd be able to decompose the continuous stream into different parts (voice, drums, ...)
%\item But this ability to understand higher level concepts is not something that machines have, where inputs are just arrays of numbers (we also wouldn't have this with a printed list of numbers).
%\item One of the key features of human perception is the ability to understand the world at different scales of detail. For instance, (image of car) is at its simplest just a car. But a car consists of doors, windows, a wind shield, wheels. Each of these components can be more closely inspected: each wheel has the metal central part and rubber tyres, and we know that out of view there is a complicated steering mechanism connects the wheels to the rest of the car. If we inspected the tyres closely we might have interesting things to say about the tread, and so on. Similarly, an album of music consists of songs that are related, each song consists of chorus and verse, within each of these there is a progression of chords, and so on. Chapter 3 presents the first major topic of my PhD, which considers how to mathematically describe the fact that there is no one objective level at which we understand any system; rather, we are aware that any understanding exists at some particular scale, and that depending on what we are doing or trying to achieve, thinking in more or less detailed ways may be appropriate.
%\item Another feature of human perception is our ability to synthesise together different streams of perceptual information into one conscious experience. For example, each of our eyes sees a 2D image. Yet we perceive the world in 3D because our brains automatically merge these two distinct streams together. (This is chapter 4, ICA stuff)
%\item When we look at a red car, we are able to understand that the 'redness' and the 'carness' are two independent features: same same car in green is fundamentally still the same kind of car even though the colour is different, while a red jumper shares little in common with the car, despite having the same colour. Moreover, if I were to present you with a picture of a red car and a green jumper, you could probably imagine what the car would look like in the jumper's shade of green, and also what the jumper would look like in the car's shade of red. Roughly speaking, this is the topic of Chapter 5, which considers a family of methods known as \emph{Wasserstein Autoencoders}.
%\item The topic of Chapter 6 is more difficult to explain by analogy to common human experience, since it is a more focussed and technical contribution to the field. The use of Wasserstein Autoencoders as in Chapter 5 requires solving a particular mathematical problem. Usually when this problem is encountered, it is very difficult to solve. In Chapter 6 we study this problem in the specific case of Wasserstein Autoencoders in great detail, and show that in this case it is actually not so hard to solve. 
%\end{itemize}
%
%
%\section{Technical introduction}
%
%\texttt{%This is the 'proper' introduction.
%%
%\begin{itemize}
%\item A human looking at an image understands its content not in terms of the pixels that are directly observed, but at higher conceptual levels such as that objects exist and relate to one another. This understanding can fluidly shift between multiple scales, so that most objects can be decomposed into smaller objects in a hierarchical fashion. Different sensory streams can be merged into a single richer conscious experience, so that we perceive the world in three dimensions despite each of our eyes seeing only in two dimensions. 
%The fact that these happen is a consequence both of evolution as well as a life-time of experience.
%\item Machine learning models, in contrast, generally have neither of these from which to benefit. This is a thesis about how high-level concepts and representations can nonetheless be modelled and learned. It examines three different ways in which representations occur, though there are others that are not treated here. 
%\end{itemize}
%
%Causality:
%
%\begin{itemize}
%\item The first of these comes from the field of \emph{causal inference}, the goal of which is to learn causal relations between random variables from either observational or experimental data. The asking of causal questions is ubiquitous in the social and natural sciences. Does smoking cause cancer? Does cutting corporation taxes cause economic growth? Will a sugar tax reduce the prevalence of obesity?
%\item It is troubling, however, that scientists often seek to discover causal relations that are ill-defined in the real world. For instance, do blood cholesterol levels causally influence the risk of heart disease? 
%\item For a long time, researchers investigated this question to find contradictory conclusions. Some found that raising cholesterol levels caused increased risk of heart disease, while others found the opposite. The resolution of these conflicting results came with the realisation that there are two types of blood cholesterol with opposite effects on heart disease risk. Thus, raising one type protects against heart disease, while raising the other raises its risk, yet both of these interventions would be registered as an increase to blood cholesterol levels.
%\item Even when the variables under investigation are well-defined, in many cases a non-trivial set of assumptions is implicitly made. Consider the link between smoking and prevalence of cancer. In reality, the human body is a complex time-evolving system consisting of numerous individual cells. The effect of regular smoking is the accumulation of small perturbations to this complex system which collectively lead to increased risk that, at some point in time, one or more of the many cells malfunction and a cancerous growth appears. 
%\item Thus, although it is true to say that "smoking causes cancer", hiding behind this simple statement is a very complicated causal relationship at the level of molecules and cells.
%\item The first contribution of this thesis is the formalisation of a theory of \emph{causal abstraction} which builds on the mathematical language for modelling causal structure known as Structural Equation Models (SEMs), providing an understanding of when it is legitimate to model causal relationships in the world at a coarser level of detail than the `true' level at which causal relations hold. The main outcome of this research was the paper \emph{Causal Consistency of Structural Equation Models} which is adapted and presented in Chapter 3. A follow-up paper, \emph{From Deterministic ODEs to Dynamic Structural Causal Models}, is not presented in this thesis. %Additionally, another causality paper, though not on the theme of causal abstractions, \emph{Probabilistic Active Learning of Functions in Structural Causal Models}, is also not presented in this thesis.
%\end{itemize}
%
%Non-linear ICA:
%
%\begin{itemize}
%\item The second type of representation comes from \emph{Independent Component Analysis} (ICA). 
%\item The \emph{cocktail party problem} is often used to describe this problem: at a party where many people are speaking, listening to the voice of a single person requires the separation of many mixed audio signals.
%\item In the classical ICA problem setting, independent sources $\mathbf{s}$ are mixed through an unknown function $f$ giving rise to a vector of observations $\mathbf{x} = f(\mathbf{s})$. Given the observations $\mathbf{x}$, the goal is to learn a function $g$ that inverts $f$ up to possibly tolerable ambiguities, thus approximately recovering the original independent sources $\mathbf{s}$. 
%\item The resulting $g(\mathbf{x})$ is a representation of the observed data that should have more appealing properties compared to the raw observations themselves.
%\item In the usual single view setting, very strong assumptions need to be made on $f$ and $\mathbf{s}$ in order to prove that recovery is possible. 
%\item The contribution of this thesis to the ICA literature is to consider a problem setting in which multiple different \emph{views} of the same sources is given. For example, different functions $f_1$ and $f_2$ give rise to different observations $f_1(\mathbf{x})$ and $f_2(\mathbf{x})$ which can both be used to recover $\mathbf{s}$. In this multi-view setting, it is proven that recovery of $\mathbf{s}$ can be made under much weaker assumptions compared to the single-view setting. This significantly increases the applicability of ICA methods, since the multi-view setting arises naturally in many real scenarios where, for example, different data modalities (audio, vision) of the same system being observed may exist.
%\end{itemize}
%
%WAE:
%
%\begin{itemize}
%\item The third and final type of representation comes from \emph{Wasserstein Autoencoders} (WAEs), a type of generative model. 
%\item Autoencoders are a broad class of method that enable learning low-dimensional representations of high-dimensional raw data. An encoder $e$ mapping from the data space $\mathcal{X}$ to the low dimensional representation space $\mathcal{Z}$ and generator or decoder $g$ mapping the reverse direction are simultaneously learned by minimising a reconstruction loss $L\left(x, g(e(x))\right)$. The result is that $e(x)$ is a compressed representation of the original data $x$. 
%\item WAEs additionally specify a \emph{prior} distribution over the latent space and impose a regularisation term that penalises deviation from this by the \emph{aggregate posterior}, the distribution of encoded data, with respect to some divergence or distance on distributions.
%\item WAEs can thus be used as a generative model, since samples can be generated by sampling from the prior and passing them through the generator, as well as a way to get representations.
%\item This thesis contains two contributions to the study of WAEs. The first is the proposal to use \emph{probabilistic} encoders that map a single datum to a distribution over latent codes. It is shown that this leads to improved generative modelling performance as well as better properties for the learned representations.
%\item The second is to analyse in detail the problem of estimating the deviation between the prior and aggregate posterior distributions for a family of divergences known as \emph{$f$-divergences}.
%\end{itemize}
%%
%%
%%Scrap:
%%
%%Unify notation: $x$ or $\mathbf{x}$ for data, $z$ or $\mathbf{z}$ for latent space. Encoder $e$ and generator $g$. 
%
%\clearpage

A human looking at an image understands its content not in terms of the pixels that are directly observed, but at higher conceptual levels.
For instance, we understand that objects exist and relate to one another. 
This understanding can fluidly shift between multiple scales, so that most objects can be decomposed into smaller objects in a hierarchical fashion. 
Different sensory streams can be merged into a single richer conscious experience, so that we perceive the world in three dimensions despite each of our eyes seeing only in two dimensions. 
The fact that this happens is a consequence both of evolution as well as a lifetime of experience.
Machine learning models, in contrast, generally have neither of these from which to benefit,
and in spite of the significant advances that have been made in recent years, the modelling of structured data remains a broad and active area of research.

This thesis considers three different areas of machine learning concerned with the modelling of data, extending theoretical understanding in each of them.
These areas are:
\begin{enumerate}
\item Autoencoders, a family of generative models, the goal of which is to model the unknown distribution of data given \iid~samples;
\item Independent Component Analysis, the goal of which is to unmix or separate signals from independent sources that have been mixed together; and
\item Causality, a broad area concerned with the modelling and inference of causal relations between random variables.
\end{enumerate}
%The contribution of this thesis to each of these areas is outlined next.
Autoencoders are a family of models that involve the introduction of a \emph{latent space} and associated \emph{encoder} mapping from the data space to this and \emph{generator} mapping in the reverse direction. 
In recent years, substantial advances have been made in this area as part of the general progress in machine learning and deep learning in particular.
Despite this, fundamental questions remain.

The estimation and minimisation of divergences between distributions in the latent spaces of autoencoders are important problems in modern research.
Of particular interest are divergences between distributions known as the \emph{prior} and \emph{aggregate posterior}, the former being a user-specified distribution and the latter being induced by the unknown data distribution in conjunction with the learned encoder.
Chapter \ref{chapter:latent-space-learning-theory} presents and studies an estimator for a class of divergences known as \emph{$f$-divergences}, deriving bounds on the rate of decay of the bias and finite sample concentration bounds.
Although this estimator may be applied in other settings, the structural assumptions required for its use are naturally satisfied in the estimation of divergences between priors and aggregate posteriors in the autoencoder setting. 


Much of the literature on $f$-divergence estimation considers settings in which weak knowledge is assumed about the distributions for which the divergence is being estimated. 
In such settings, the number of samples required to estimate the divergence typically grows exponentially in the dimension of the space over which the distributions are defined, unless the associated densities satisfy strong smoothness assumptions that are difficult to verify in practice.
By exploiting the natural structure present in the autoencoder setting, superior rates are obtained in Chapter \ref{chapter:latent-space-learning-theory} with only mild and easily verifiable additional assumptions.
These results additionally have implications to existing work elsewhere in the literature by giving a rigorous foundation to heuristic proposals in related settings.


Independent Component Analysis (ICA) assumes that data are generated by independent sources that are mixed together. 
This is formalised by introducing a latent space over which a factorised \emph{source distribution} is assumed. 
Observations are obtained by passing the sources through a \emph{mixing function}, each coordinate of which is a function of several sources.
Given a dataset of observations, the goal of ICA is to invert the unknown mixing function and recover the independent sources.

In addition to the derivation of practical algorithms, one of the main lines of enquiry in the ICA community is the search for \emph{identifiability results}.
Specifying an ICA problem requires making assumptions, for instance on the source distribution or the mixing functions, thus defining a restricted family of models.
An ICA problem is \emph{unidentifiable} if there are multiple fundamentally different models in this restricted family that result in the same data distribution, and is \emph{identifiable} otherwise.
%For instance, taking a model and permuting the coordinates of the latent variables results in a new model that is fundamentally similar to the first, while linearly mixing the latent variables makes them fundamentally dissimilar. 
Identifiability results allow us to understand conditions under which it is in principle possible---or impossible---to recover the latent sources up to tolerable ambiguities.


At the one extreme of very strong assumptions, identifiability holds if the mixing functions are linear and at most one component of the latent sources is Gaussian. 
At the other extreme, the ICA problem is unidentifiable if no assumptions on the mixing functions or sources are made. 
This line of research thus seeks to identify settings between these extremes where identifiability still holds. 
Chapter \ref{chapter:ica} extends the scarce literature of identifiability results for \emph{nonlinear ICA} by considering a novel setting in which \emph{multiple views} of the sources through different mixing functions are available.
In particular, if one noiseless view of the sources is supplemented by a second view that is appropriately corrupted by source-level noise, the sources can be fully reconstructed from the observations up to tolerable ambiguities.
These theoretical results have important practical implications, as in many applications such as neuroimaging, multiple data modalities may be simultaneously available.
These results show that jointly using these different modalities can result in recovery of the sources under weaker conditions than when using them separately.

The field of causality is largely concerned with the inference of causal relationships between variables from data. 
These are distinct from statistical relationships, since if two variables are statistically dependent, either one may causally influence the other or they could both be influenced by a third.
Causal inference algorithms generally operate on vectorial data in which each component is assumed to be a variable that exhibits causal relations with some subset of the other components, the goal then being identify these relations.
But in many realistic scenarios, there is no guarantee that the components of collected data are well-defined causal variables in this sense.

For example, early investigations into the influence of blood cholesterol on the risk of developing heart disease found conflicting results.
Some studies found that raising total blood cholesterol levels increased the risk, while others found the opposite. 
Later, it was discovered that there are in fact two types of cholesterol, one reducing the risk, the other raising it. 
Attempting to identify the causal relation between their \emph{sum} and the risk was therefore doomed to failure: total blood cholesterol is not a well-defined causal variable in relation to the risk of heart disease.

More generally, though any physical system in the real world exhibits causal structure at some level of detail, measurements are typically made at a more coarse-grained or abstract level. 
Chapter \ref{chapter:causality} seeks to identify when these `higher-level' variables admit an interpretation as well-defined causal variables.
This is done by introducing a framework for understanding when two causal models are consistent with one another: if a high-level model is consistent with a low-level model that is physically grounded in real causal relations, it admits a causal interpretation through its connection to the low-level model.
This work has implications to the causal modelling generally by giving a handle on the problem of defining causal variables. 


%This is a thesis about how high-level concepts and representations can nonetheless be modelled and learned. It examines three different ways in which representations occur, though there are others that are not treated here. 

%Much of machine learning concerns the modelling of structured data. 

\section{Outline and Contributions}

This thesis is organised as follows.

\begin{itemize}
\item Chapter \ref{chapter:literature} is an overview of the literature on generative modelling with Latent Variable Models (LVMs), providing important background for Chapters \ref{chapter:latent-space-learning-theory} and \ref{chapter:ica}. 
This defines and discusses LVMs, divergences including the family of $f$-divergences, density ratio estimation and examples of generative models including autoencoders.
\item Chapter \ref{chapter:latent-space-learning-theory} presents and studies an estimator for $f$-divergences between distributions with particular application to the setting of autoencoders. 
The natural structural assumptions that hold in this setting make it possible to estimate the divergences with fast rates.
In contrast, in much of the existing $f$-divergence estimation literature, fast rates are only attainable with strong assumptions that would be hard to verify in practice.
This chapter is based on \cite{rubenstein2019practical}.
\item Chapter \ref{chapter:ica} presents novel identifiability results for nonlinear ICA, extending the scarce literature of such results. 
A multi-view setting is considered in which multiple observations of the sources are simultaneously available through different mixing functions.
In particular, supplementing a noiseless view of the sources with a second appropriately corrupted view leads to the model being identifiable.
This has application to practical scenarios in which multiple data modalities are available.
This chapter is based on \cite{gresele2019incomplete}.
\item Chapter \ref{chapter:causality} presents a framework for understanding when two casual models at different levels of detail are consistent with one another, showing how high-level causal variables can arise as functions of lower-level variables.
This has implications to the understanding of causal modelling by shedding light on the definition of causal variables, and in particular highlights the importance of considering interventions as part of the causal modelling process.
This chapter is based on \cite{rubenstein2017causal}.
\item Chapter \ref{chapter:conclusion} concludes and additional materials for the main chapters are included in Appendices \ref{chapter:appendix-latex-space-learning-theory}, \ref{chapter:appendix-ica} and \ref{chapter:appendix-causality}.
\end{itemize}

\medskip

All of the work presented in this thesis was conducted with my numerous collaborators. 
The conference papers cited above on which the chapters of this thesis are based are repeated with a full list of collaborators:

\medskip


\begin{quote}
\fullcite{rubenstein2019practical}

\medskip

\fullcite{gresele2019incomplete_fullcite}

\medskip

\fullcite{rubenstein2017causal_fullcite}
\end{quote}

\medskip

In addition, the following papers were also written during my PhD but are not discussed in this thesis.

\begin{quote}
\fullcite{rubenstein2018deterministic}

\fullcite{rubenstein2017probabilistic}

\fullcite{rubenstein2018learning}

\fullcite{rubenstein2018wasserstein}

\fullcite{rubenstein2018latent}

\fullcite{rubenstein2018empirical}

\fullcite{tschannen2020onmutual}

\fullcite{janzing2018structural}

\fullcite{von2019optimal}
\end{quote}























