%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Second Chapter *********************************
%*******************************************************************************

\chapter{Generative Modelling with Latent Variable Models}

\ifpdf
    \graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
\else
    \graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi

\emph{This chapter introduces key ideas in the literature of generative modelling with latent variable models relevant to this thesis:}
\emph{Chapter \ref{chapter:latent-space-learning-theory} presents learning theoretic results for divergence estimation in the latent spaces of autoencoders, a type of latent variable generative model;}
\emph{Chapter \ref{chapter:ica}, which concerns Independent Component Analysis (ICA), presents identifiability results for a particular class of latent variable model.}


\section{Introduction}\label{sec:generative-modelling-tour}

Suppose that a dataset of samples, drawn \iid~from some unknown data distribution $Q_X$, is given.
The high level goal of generative modelling is to learn a model distribution $P_X$ that approximates the unknown data distribution $Q_X$ based on these samples.
The applications of generative modelling are diverse, since
having such a model of the data may be desirable for a variety of reasons, for instance: to artificially generate new samples of data \citep{goodfellow2014generative, oord2016wavenet}; to perform compression \citep{townsend2019practical, townsend2019hilloc}; to unsupervisedly learn features for transfer to other tasks \citep{tschannen2018recent, donahue2019large}; or to extract latent structure present in the data \citep{hyvarinen2000independent}.
Both Chapters \ref{chapter:latent-space-learning-theory} and \ref{chapter:ica} of this thesis directly concern generative models, and the goal of this chapter is to introduce the required background.


%Generative modelling has in recent years become synonymous with the artificial generation of natural images or audio \citep{goodfellow2014generative, oord2016wavenet}.
%The mathematical formulation of this problem is, however, more general, and has close connections to other fields such as Independent Component Analysis (ICA) that will be covered in Chapter \ref{chapter:ica}.



% from which new samples can be drawn. 
To make the problem of generative modelling precise, one must specify a choice of divergence
%\footnote{Defined in Section \ref{subsec:gen-model-divergence}} 
$D$ and family of distributions $P_X^\theta$ with parameter $\theta\in\Theta$, and the goal becomes:
%
\begin{align*}
\min_{\theta \in \Theta} \ D\left(P_X^\theta,  Q_X \right)
\end{align*}
%
There are two main challenges in practically implementing and solving this problem.
First, when the data are drawn from complex high dimensional distributions, how can this be modelled with a parameterised distribution that is computationally tractable?
Second, what are appropriate choices of divergences, and how can they be estimated or minimised with respect to the parameters $\theta$?
These questions will be discussed in the rest of this chapter. 

%One of the main families of methods for solving this problem, autoencoders, involve the introduction of a latent space and encoder, a mapping from the data to the latent space.
%The main contribution of Chapter \ref{chapter:latent-space-learning-theory} is a learning theoretic analysis of the estimation of divergences in this latent space. 


\section{Latent variable models}

A Latent Variable Model (LVM) is a way to specify complex distributions over potentially high-dimensional spaces using simple components. 
These are flexible models that are used widely in the machine learning literature. 
As such, different parts of the literature often use different terminology to describe fundamentally similar ideas. 
In the following, two sets of nomenclature will often be introduced, corresponding to those used in the GAN/VAE community, mostly relevant to Chapter \ref{chapter:latent-space-learning-theory}, and those used in the ICA community, mostly relevant to Chapter \ref{chapter:ica}.

\medskip

\begin{definition}[Latent Variable Model]
A Latent Variable Model (LVM) over a space $\mathcal{X}$ consists of a distribution $P_Z$ over a low dimensional space $\mathcal{Z}$ together with a conditional distribution $P_{X|Z}$. 
Together, these induce a distribution $P_X$ over the data-space. 
\end{definition}

%In practice, an LVM may be parametrised, either by parametrising the prior as $P_Z^\theta$, or the conditional distributions as $P_{X|Z}^\theta$, or both.


$P_Z$ is often referred to as a \emph{prior} or \emph{noise distribution} and is in many cases fixed to be some simple distribution such as a unit Gaussian or uniform distribution. 
In other cases, such as in Independent Component Analysis (ICA; see Chapter \ref{chapter:ica}), $P_Z$ is referred to as a \emph{source distribution} and may be specified only implicitly through some assumed properties, such as being a factorised distribution.

The conditional distributions can be thought of as a mapping $g:\mathcal{Z}\to\mathcal{P}(\mathcal{X})$ from elements of $\mathcal{Z}$ to distributions over $\mathcal{X}$,
and may in general be dirac-delta distributions, in which case the associated mapping $g$ is well-defined as a function $g:\mathcal{Z}\to\mathcal{X}$. 

The conditional distributions are usually referred to as \emph{generators} or \emph{decoders} in the literature, and when given parameter $\theta$ may be written as $P_{X|Z}^\theta$ or $g^\theta$. 
If the conditional distributions they correspond to are dirac-delta distributions, the generators are called \emph{deterministic}, otherwise they are \emph{stochastic}.
In the ICA literature, the generators are always deterministic and are known as \emph{mixing functions}, and are usually denoted by $f$.

In practice, generators are usually implemented as a deep neural network.
This is straightforward in the deterministic case: $g^\theta$ is simply a function and can thus be represented as a deep network with parameters $\theta$.
If the generator is stochastic, it can still be represented simply as a deep network provided that the conditional distributions are sufficiently structured. 
For instance, if the conditional distributions are Gaussian with varying mean and covariance, $g^\theta$ can be represented as a deep network with two outputs, one for the mean and one for the covariance.

For a fixed choice of parameter $\theta$, $P_Z$ and $P^\theta_{X|Z}$ specify a joint distribution $P^\theta_{XZ}$ over $\mathcal{X} \times \mathcal{Z}$ and thus a distribution distribution $P_X^\theta$ over the data-space $\mathcal{X}$. 
For simplicity, we will assume that densities of all relevant distributions exist, so that this is equivalent to specifying $P_X^\theta$ via the integral
%
\begin{align*}
p^\theta(x) = \int p^\theta(x|z) p(z) dz.
\end{align*}
%
This integral will generally be intractable, meaning that $P_X^\theta$ has unknown density in practice.
However, the important reason that LVMs are useful is that samples from $P_X^\theta$ can be drawn easily:
one samples first a value $z\sim P_Z$ and then $x \sim P^\theta_{X|Z=z}$. 
All relevant distributions can be chosen so that these sampling procedures are simple, e.g.\:if $P_Z$ and all $P^\theta_{X|Z}$ are Gaussian. 


\subsection{Integral Probability Metrics and $f$-divergences}\label{subsec:gen-model-divergence}

A \emph{divergence} is a notion of dissimilarity between pairs of distributions. 
Denoting by $\mathcal{P}_{\mathcal{X}}$ the set of distributions on $\mathcal{X}$, a divergence $D$ is a mapping $D: \mathcal{P}_{\mathcal{X}} \times \mathcal{P}_{\mathcal{X}} \to \mathbb{R} \cup \{\infty\}$ such that

\begin{itemize}
\item $D(P, Q)  \geq 0$ for any distributions $P, Q \in \mathcal{P}_{\mathcal{X}}$,
\item $D(P, Q) = 0$ if and only if $P = Q$.
\end{itemize}

This notion is weaker than that of a \emph{metric} in that a divergence need not be symmetric or obey the triangle inequality.
For a divergence $D(P^\theta_X, Q_X)$ to be useful in the context of generative modelling, it must be possible to minimize it with respect to the parameters $\theta$. 
We will discuss in the next section how this can be done, discussing here the two main families 
of divergences that occur in the machine learning literature generally, and the generative modelling literature in particular. 

\subsubsection{Integral Probability Metrics (IPMs)}\label{subsec:intro-ipm}


IPMs are divergences that can be written as
%
\begin{align*}
D_{\mathcal{H}}(P, Q) = \sup_{h\in\mathcal{H}} \left| \int h(x) dP(x) - \int h(x) dQ(x) \right|
\end{align*}
%
for some restricted function class $\mathcal{H}: \mathcal{X} \to \mathbb{R}$. 
If $\mathcal{H}$ is too small, $D_\mathcal{H}$ may not be a divergence.
For instance, taking $\mathcal{H}$ to contain only the constant $0$ function results in $D_{\mathcal{H}}(P, Q) = 0$ for any $P$ and $Q$.
On the other hand, if $\mathcal{H}$ is too rich then $D_{\mathcal{H}}$ may not be useful: 
taking $\mathcal{H}$ to be the set of all real-valued measurable functions leads to the trivial (Theorem 1, \cite{sriperumbudur2009integral})
\begin{align*}
D_{\mathcal{H}}(P, Q) = \begin{cases} 0 &\text{ if } P = Q \\ \infty &\text{ if } P \not= Q\end{cases}
\end{align*}
%
which would in practice not be useful since a degree of smoothness in $P$ and $Q$ is normally desirable.
Provided $\mathcal{H}$ is sufficiently rich that $D_{\mathcal{H}}$ is a divergence, it is in fact a metric, obeying the triangle inequality and symmetry.
These properties are inherited from the function $d(x,y) = |x - y|$.
%For symmetry, observe that for any $h$, 
%%
%\begin{align*}
%\left| \int h(x) dP(x) - \int h(x) dQ(x) \right| = \left| \int h(x) dQ(x) - \int h(x) dP(x) \right|,
%\end{align*}
%%
%and thus taking the supremum over $h \in \mathcal{H}$ yields $D_{\mathcal{H}}(P, Q) = D_{\mathcal{H}}(Q, P)$. 
%For the triangle inequality


Commonly encountered IPMs include Wasserstein or optimal transport distances (where $\mathcal{H}$ is the set of all functions with Lipschitz constant $1$ with respect to some metric), the Total Variation distance (where $\mathcal{H}$ is the set of all functions with infinity norm $1$) and the Maximum Mean Discrepancy (where $\mathcal{H}$ is the set of functions in a reproducing kernel Hilbert space with norm at most $1$ induced by some kernel $k$ \cite{gretton}). 
Optimal transport distances will be discussed further in the section on Wasserstein Autoencoders, making use of a different formulation of these distances than as IPMs.

\subsubsection{$f$-divergences}

%\begin{definition}[$f$-divergence]
%\label{def:fdiv}
%Let $f$ be a convex function on $(0, \infty)$ with $f(1) = 0$. 
%The $f$-divergence $D_f$ between distributions $Q_Z$ and $P_Z$ admitting densities $q(z)$ and $p(z)$ respectively is
%{\addtolength{\abovedisplayskip}{-0.5mm}
%\addtolength{\belowdisplayskip}{-0.5mm}
%\begin{align*}
%    D_f(Q_Z \| P_Z) := \int f \left( \frac{q(z)}{p(z)} \right) p(z) dz.
%\end{align*}}%
%\end{definition}
%Many commonly used divergences such as Kullback–Leibler and $\chi^2$ are $f$-divergences.
%All the divergences considered in this paper together with their corresponding $f$ can be found in Appendix~\ref{appendix:f-fns}. 
%Of them, possibly the least well-known in the machine learning literature are $f_\beta$-divergences \cite{osterreicher2003new}. 
%These symmetric divergences are continuously parameterized by $\beta\in(0, \infty]$. Special cases include squared-Hellinger ($\mathrm{H}^2$) for ${\beta=\frac{1}{2}}$,  Jensen-Shannon (JS) for $\beta=1$, Total Variation (TV) for $\beta=\infty$. 

%\todo{flesh out this section, give more properties of $f$-divergences?}

Given a convex real-valued function $f$ defined on $(0, \infty)$ such that $f(1)=0$, the $f$-divergence between two distributions $P$ and $Q$ with densities $p(x)$ and $q(x)$ is
%
\begin{align*}
D_f(P, Q) = \int f\left(\frac{p(x)}{q(x)}\right) q(x) dx.
\end{align*}
%
This is defined for distributions $P$ and $Q$ for which $P$ is absolutely continuous with respect to $Q$, meaning that the density ratio $p(x)/q(x)$ is finite for all $x$, and is usually taken to be $\infty$ otherwise.
The fact that $f(1)=0$ implies that $D_f(P,P) = 0$ for any $P$.
Positivity of $D_f$ follows from convexity of $f$ and Jensen's inequality:
%
\begin{align*}
D_f(P, Q) &= \int f\left(\frac{p(x)}{q(x)}\right) q(x) dx \\ 
&\geq f\left(\int \frac{p(x)}{q(x)} dx\right) \\
&= f(1) = 0.
\end{align*}
%
For any $f$ that is strictly convex, the above inequality is strict if $P\not=Q$.
This can similarly be shown for any $f$ for which there exists some $c$ such that $f(u) > c(u-1)$ for all $u\not=1$. 
Indeed, for any $P \not= Q$ there exists a set $A \subset \mathcal{X}$ with positive measure under $Q$ on which $p(x)/q(x) \not= 1$ (if this were not the case, we would have $p(x)/q(x) = 1$ everywhere that $Q$ puts mass and so $P=Q$).
Thus,
\begin{align*}
D_f(P, Q) &= \int_A f\left(\frac{p(x)}{q(x)}\right) q(x) dx +  \int_{\mathcal{X} \setminus A} f\left(\frac{p(x)}{q(x)}\right) q(x) dx\\ 
&> \int_A c \left(\frac{p(x)}{q(x)} - 1\right) q(x) dx +  \int_{\mathcal{X} \setminus A} c \left(\frac{p(x)}{q(x)} - 1\right) q(x) dx\\ 
&= 0.
\end{align*}

By selecting different choices for $f$, several commonly encountered divergences can be recovered, including the Kullback-Leibler, Jensen-Shannon, Total Variation, $\chi^2$ and $\alpha$-divergences as well as the lesser known $\beta$-divergences \citep{osterreicher2003new}.
These symmetric divergences are continuously parameterized by $\beta\in(0, \infty]$. Special cases include squared-Hellinger ($\mathrm{H}^2$) for ${\beta=\frac{1}{2}}$,  Jensen-Shannon (JS) for $\beta=1$, Total Variation (TV) for $\beta=\infty$. 

One of the useful properties of $f$-divergences is that for any constant $c$, replacing $f(u)$ by $\tilde{f}(u) = f(u) + c(u-1)$ does not change the divergence $D_f$. 
To see this, observe that
%
\begin{align*}
D_{\tilde{f}}(P, Q) &= \int \left[ f\left(\frac{p(x)}{q(x)}\right) + c\left(\frac{p(x)}{q(x)} - 1\right) \right] q(x) dx \\
&=  \int f\left(\frac{p(x)}{q(x)}\right) q(x) dx + c \int p(x) - q(x) dx \\
&= D_f(P, Q)
\end{align*}
%
since $p(x)$ and $q(x)$ integrate to $1$.
It is often convenient to work with $f_0(u) := f(u) - f'(1)(u-1)$ which is decreasing on $(0, 1)$ and increasing on $(1, \infty)$ and satisfies $f'_0(1)=0$.
In Table \ref{table:f-fns} we list the forms of the function $f_0$ for each of the divergences considered in this chapter.



%\section{$f$ for divergences considered in this paper}\label{appendix:f-fns}



{
\renewcommand{\arraystretch}{2}
\begin{table}
 \caption{$f$ corresponding to divergences referenced in this chapter.}
 \label{table:f-fns}
 \centering
 \begin{tabular}{c c} 
 \toprule
 $f$-divergence & $f_0(x)$ \\
 \midrule
 KL & $x \log x - x + 1$\\
 TV & $\frac{1}{2}|1-x|$\\
 $\chi^2$ & $x^2 - 2x$\\
 $\text{H}^2$ & $2(1-\sqrt{x})$\\
 JS & $(1+x)\log(\frac{2}{1+x}) + x\log x$\\
 $D_{f_\beta}$, $\beta > 0,$ $\beta\not=\frac{1}{2}$ & $\frac{1}{1-\frac{1}{\beta}}\left[ (1+x^\beta)^{\frac{1}{\beta}} - 2^{\frac{1}{\beta}-1}(1+x) \right]$\\
 $D_{f_\alpha}$, $-1<\alpha < 1$ & $\frac{4}{1-\alpha^2}\left( 1 - x^{\frac{1+\alpha}{2}} \right) - \frac{2(x-1)}{\alpha-1}$ \\
 \bottomrule
\end{tabular}
\end{table}
}


%\begin{align*}
%D_f(P, Q) = 
%    \begin{cases}
%       \int f\left(\frac{dQ}{dP}(x) \right) dP, & \text{if $dQ/dP$ exists} \\
%      \infty, & \text{otherwise}
%    \end{cases}
%\end{align*}

%Intuitively, such divergences measure the discrepancy between the two distributions by seeking functions which are maximal on one distribution and minimal on the other. 


$f$-divergences also admit a variational form as a result of convex conjugacy \citep{nguyen10ratio}.
Any convex function $f(u)$ has a conjugate $f^*(t)$ defined as
%
\begin{align*}
f^*(t) = \sup_{u \in \dom(f)} \{ut - f(u)\}.
\end{align*}
%
The resulting function $f^*$ is itself convex, and
provided that $f$ is continuous, $f$ and $f^*$ are dual in the sense that $f^{**} = f$.
This means that $f$ can be written as
%
\begin{align*}
f(u) = \sup_{t \in \dom(f^*)} \{ut - f^*(t)\}.
\end{align*}
%
Plugging this into the definition of $f$-divergences yields
%
\begin{align*}
D_f(P, Q) &= \int q(x) \sup_{t \in \dom(f^*)} \left\{t \frac{p(x)}{q(x)} - f^*(t) \right\} dx \\
& \geq \sup_{T \in \mathcal{T}} \left\{ \int p(x) T(x) dx - \int q(x) f^*(T(x)) \right\} dx \\
&= \sup_{T \in \mathcal{T}} \mathbb{E}_{x \sim P} \left[ T(x) \right] - \mathbb{E}_{x \sim Q} \left[ f^*(T(x)) \right] 
\end{align*}
where $\mathcal{T}$ is an arbitrary class of functions $\mathcal{X} \to \dom(f^*) \subseteq \mathbb{R}$.
In this chapter we do not make use of this variational formulation of $f$-divergences, but it will be used in explaining Generative Adversarial Networks in the next section.

\subsection{Examples of generative models}

To actually solve the generative modelling problem, it must be possible to minimise $D(P^\theta_X, Q_X)$ with respect to the parameters $\theta$ in a computationally tractable way.
In general it is infeasible to estimate $D(P^\theta_X, Q_X)$ directly, or even to compute gradients of it with respect to $\theta$. 
Thus, one must resort to approximations.


There are two main families of methods that introduce auxiliary functions as computational tricks to bound or estimate $D(P^\theta_X, Q_X)$ in a computationally tractable way.
These are \emph{Generative Adversarial Networks (GANs)}, which introduce a \emph{discriminator} $d:\mathcal{X} \to \mathbb{R}$, and \emph{autoencoders}, which introduce an \emph{encoder} $e:\mathcal{X} \to \mathcal{Z}$.

In the following subsections we discuss GANs and two types of autoencoders, Variational Autoencoders (VAEs) and Wasserstein Autoencoders (WAEs), showing how specific choices of divergences can be approximated.

\subsubsection{Generative Adversarial Networks (GANs)}

GANs are a family of methods that approximately minimise any $f$-divergence and some choices of IPMs. 
\cite{goodfellow2014generative} introduced the idea of training a discriminator $d^\phi: \mathcal{X} \to \mathbb{R}$ to distinguish between `real' samples from $Q_X$ and `fake' samples from $P^\theta_X$, which then provides a surrogate loss for a generator $g^\theta$, trained simultaneously to maximise the loss of the discriminator. 
In \cite{goodfellow2014generative}, the loss of the discriminator is implemented as logistic regression with the output $d^\phi(x)$ taken to be the logit, so that the loss is
%
\begin{align*}
L(\theta, \phi) = \mathbb{E}_{x\sim Q_X}\left[ \log d^\phi(x) \right] - \mathbb{E}_{x \sim P_Z} \left[\log(1 - d^\phi(g^\theta(z)) \right].
\end{align*}
%
Stochastic gradients of this loss can be taken with respect to both $\phi$ and $\theta$ by using minibatch samples of data to approximate the outer expectations when $d^\phi$ and $g^\theta$ are both neural networks.
It was shown that this loss satisfies $L(\theta, \phi) \leq 2 \cdot D_{\text{JS}}(P^\theta_X, Q_X) - \log 4$ for any $d^\phi$ and $g^\theta$, with equality attainable only if the class of $d^\phi$ is sufficiently rich, where $D_{\text{JS}}$ is the Jensen-Shannon divergence.

Thus, although actually computing $D_{\text{JS}}(P^\theta_X, Q_X)$ is intractable, the surrogate loss $L(\theta, \phi)$ is (up to constants) a lower bound on this.
Hence the Jensen-Shannon divergence can be approximately minimised by \emph{maximising} $L(\theta, \phi)$ with respect to $\phi$ and minimising this with respect to $\theta$.

\cite{nowozin2016f}, building on the work of \cite{nguyen10ratio}, generalised this result to arbitrary $f$-divergences using the variational formulation of $f$-divergences.
%
\begin{align*}
D_f(P, Q) &\geq \sup_{T \in \mathcal{T}} \mathbb{E}_{x \sim P} \left[ T(x) \right] - \mathbb{E}_{x \sim Q} \left[ f^*(T(x)) \right] 
\end{align*}
%
where the set of functions $\mathcal{T}$ is implemented as a neural network.
Thus, parameterising $T$ by $\phi$ yields the surrogate loss
%
\begin{align*}
L_f(\phi, \theta) = \mathbb{E}_{x \sim Q_X} \left[ T^\phi(x) \right] - \mathbb{E}_{z \sim P_Z} \left[ f^*(T^\phi(g^\theta(x))) \right] \leq D_f(P^\theta_X, Q_X)
\end{align*}
%
As with the original GAN objective, stochastic gradients of this surrogate loss can be computed. 
The function $T^\phi$ corresponds to the discriminator introduced by \cite{goodfellow2014generative}, but the above derivation holds for arbitrary choices of $f$. 
\cite{nowozin2016f} thus demonstrated how the GAN `trick' can be applied to other $f$-divergences to get computational tractable lower bounds of $D_f(P^\theta_X, Q_X)$.

A similar idea can also be used to approximately minimise IPMs $D_\mathcal{H}(P^\theta_X, Q_X)$, provided that the function class $\mathcal{H}$ can be parameterised in practice.
If $\mathcal{H}$ is such that $h \in \mathcal{H}$ implies that $-h \in \mathcal{H}$, $D_\mathcal{H}$ can be written without the inner absolute function, leading to
%
\begin{align*}
D_{\mathcal{H}}(P, Q) &= \sup_{h\in\mathcal{H}} \int h(x) dP(x) - \int h(x) dQ(x) \\
&= \sup_{h\in\mathcal{H}} \left\{ \mathbb{E}_{x \sim P} \left[ h(x) \right]- \mathbb{E}_{x \sim Q} \left[ h(x) \right] \right\} \\
&\geq  \mathbb{E}_{x \sim P} \left[ h(x) \right]- \mathbb{E}_{x \sim Q} \left[ h(x) \right] \\
\end{align*}
%
where the inequality holds for any $h \in \mathcal{H}$. 
Here $h$ plays the role of the discriminator. 
If it is differentiable, stochastic gradients of the loss can be obtained with respect its parameters.
Note that in contrast to the $f$-divergence case above, the discriminator $h$ is restricted to belong to a certain class of functions which can complicate specifying the parameterisation.
The most notable example of an IPM being used for generative modelling is the Wasserstein GAN of \cite{AB17}. 
Here, the Wasserstein distance is used, corresponding to $h$ being restricted to having Lipschitz constant at most $1$. 
For certain neural network architectures, including those composed of fully-connected and convolutional layers, this can be enforced by weight clipping.

\subsubsection{Variational Autoencoders (VAEs)}

VAEs \citep{kingma2013auto, rezende2014stochastic} are a method to minimize the KL-divergence between model and data distributions, defined as
%
\begin{align*}
D_{\text{KL}}(Q_X, P^\theta_X) &= \int q(x) \log \left( \frac{q(x)}{p^\theta(x)} \right) dx \\
&= \int q(x) \log q(x) - \int q(x) \log p^\theta(x) dx.
\end{align*}
%
The first term above, the negative of the \emph{differential entropy} of $Q_X$, often written $H(Q_X)$, cannot be estimated without knowledge of the density $q(x)$.
However, since it is constant as a function of $\theta$, it can be ignored.
The term $\log p^\theta(x)$ inside the second integral is known as the \emph{log-likelihood} or \emph{evidence}. 
Although the density $p^\theta(x)$ is intractable, the evidence can be tractably lower bounded leading to the so-called \emph{evidence lower bound} (ELBO), which in turn leads to a tractable \emph{upper} bound on $D_{\text{KL}}(Q_X, P^\theta_X)$. 

First, observe that the log-likelihood can be written 
%
\begin{align*}
\log p^\theta(x) &= \log \left( \int p^\theta(x|z) p(z) dz \right).
\end{align*}
%
Given any distribution $q^\phi(z|x)$ depending on parameter $\phi$ and the value of $x$, we can multiply and divide inside the integral, leaving its value unchanged.
This leads to
%
\begin{align*}
\log p^\theta(x) &= \log \left( \int p^\theta(x|z) \frac{p(z)}{q^\phi(z|x)} q^\phi(z|x) dz \right) \\
&= \log \left( \mathbb{E}_{q^\phi(z|x)} \left[  p^\theta(x|z) \frac{p(z)}{q^\phi(z|x)} \right]\right) \\
&\geq \mathbb{E}_{q^\phi(z|x)} \left[ \log p^\theta(x|z) + \log p(z) - \log q^\phi(z|x) \right] \\
&= \mathbb{E}_{q^\phi(z|x)} \log p^\theta(x|z)  - D_{\text{KL}}\left(q^\phi(z|x), p(z)\right),
\end{align*}
%
where the inequality follows from Jensen's inequality due to the concavity of $\log$.
The distribution $q^\phi(z|x)$ is a variational approximation to the true posterior $p^\theta(z|x)$; it can be shown that the gap introduced by Jensen's inequality is equal to $D_{\text{KL}}\left(q^\phi(z|x), p^\theta(z|x)\right)$. 
$q^\phi(z|x)$ is often referred to as an encoder as it maps elements of the data-space $\mathcal{X}$ to distributions over the latent space $\mathcal{Z}$.
Putting things together yields
%
\begin{align*}
D_{\text{KL}}(Q_X, P^\theta_X) \leq -H(Q_X)  - \mathbb{E}_{q(x)} \mathbb{E}_{q^\phi(z|x)} \log p^\theta(x|z) + \mathbb{E}_{q(x)}D_{\text{KL}}\left(q^\phi(z|x), p(z)\right)
\end{align*}
%
where $H(Q_X)$ is the constant differential entropy, leading to the VAE loss
%
\begin{align}
L_{\text{VAE}}(\theta, \phi) =  \underbrace{\mathbb{E}_{q(x)} \mathbb{E}_{q^\phi(z|x)} \left[ - \log p^\theta(x|z)\right]}_{\text{(i)}} + \underbrace{\mathbb{E}_{q(x)}D_{\text{KL}}\left(q^\phi(z|x), p(z)\right)}_{\text{(ii)}}
\end{align}
%
which (up to constants) is an upper bound on $D_{\text{KL}}(Q_X, P^\theta_X)$.
It is common for the prior to be a standard Gaussian, the generator to output Gaussians with mean $\mu^\theta(z)$ and fixed isotropic covariance, and the encoder to map to Gaussians with mean $\mu^\phi(x)$ and diagonal covariance $\Sigma^\phi(x)$.
In this case, term (i) above can be interpreted as an average reconstruction loss and (ii) as a regulariser, hence making the model a type of regularised autoencoder\footnote{Note however that the interpretation of VAEs as a sort of autoencoder breaks down when more powerful classes of generators are used, as discussed in \href{http://paulrubenstein.co.uk/variational-autoencoders-are-not-autoencoders/}{http://paulrubenstein.co.uk/variational-autoencoders-are-not-autoencoders/}.}.

The encoder $Q^\phi_{Z|X}$ together with the data distribution $Q_X$ induce the push-forward distribution $Q^\phi_Z$ known as the \emph{aggregate posterior}.
The term (ii) was shown by \cite{hoffman2016elbo} to be equivalent to $D_{\text{KL}}(Q^\phi_Z , P_Z) + I(X,Z)$ where $I(X,Z) = D_{\text{KL}}(Q^\phi_{XZ} , Q_XQ^\phi_{Z})$ is the mutual information of a data sample and its encoding.


%All distributions involved can be chosen so this can be unbiasedly estimated, meaning that it can be minimised with respect to the parameters $\theta$ and $\phi$ using stochastic gradient methods.
%It is common that the encoder, generator and prior are chosen to be Gaussians, 


\todo{discuss the fact that the aggregate posterior and prior satisfy the setting we consider and mention that some works consider divergences between them?}

\subsubsection{Wasserstein Autoencoders (WAEs)}

WAEs \citep{tolstikhin2017wasserstein} are concerned with optimal transport or Wasserstein distances.
These were discussed briefly in Section \ref{subsec:intro-ipm} as they admit a formulation as IPMs. 
In this section we will make use of a different formulation of these distances.

Let $c$ be any metric on $\mathcal{X}$.
For intuition, $c(x, x')$ may be thought of as a function specifying the cost of transporting a point from $x$ to $x'$.
The optimal transport distance between two distributions $P$ and $Q$ over $\mathcal{X}$ is then the minimal cost incurred by transporting the probability mass of $P$ to that of $Q$.
For simplicity in the following, we will assume $P$ and $Q$ have densities $p$ and $q$. 

Formally, let $\Gamma$ be the set of joint distributions over $\mathcal{X} \times \mathcal{X}$ with marginals $P$ and $Q$. 
That is, any element $\gamma(x, x') \in \Gamma$ is a joint distribution satisfying $\gamma(x) = p(x)$ and $\gamma(x') = q(x')$.
Then, the optimal transport distance is defined as
%
\begin{align*}
OT_c(P, Q) = \min_{\gamma \in \Gamma} \mathbb{E}_{x, x' \sim \gamma} \left[ c(x, x') \right].
\end{align*}
%
This can equivalently be written as
%
\begin{align*}
OT_c(P, Q) = \min_{\gamma \in \Gamma} \mathbb{E}_{x\sim Q} \mathbb{E}_{x'\sim \gamma(x'|x)} \left[ c(x, x') \right],
\end{align*}
%
where instead we think of the $\gamma$ as a conditional distribution that specifies how an element of probability mass at $x$ should be spread over the points $x'$. 
Specified this way, each element of $\Gamma$ should satisfy $\int \gamma(x'|x) q(x) dx = p(x')$.
In our case of interest, we thus have
%
\begin{align*}
OT_c(P_X^\theta, Q_X) = \min_{\gamma \in \Gamma} \mathbb{E}_{x\sim Q_X} \mathbb{E}_{x'\sim \gamma(x'|x)} \left[ c(x, x') \right].
\end{align*}
%
In practice, this minimisation problem cannot be solved directly: given a candidate conditional distribution $\gamma(x'|x)$ it is not practically possible to verify whether or not $\int \gamma(x'|x) q_X(x) dx = p_X^\theta(x')$ holds since only samples from $Q_X$ are available and the density $p^\theta_X(x)$ is intractable.

\cite{tolstikhin2017wasserstein} proved the following result, giving a handle on this problem:
If the generator $p^\theta(x|z)$ is deterministic, any valid $\gamma$ can be written as a composition $\gamma(x'|x) = \int p^\theta(x'|z) q^\phi(z|x) dz$ where $q^\phi(z|x)$ is a conditional distribution satisfying $\int q^\phi(z|x) q(x) dx = p(z)$. 
That is, any $\gamma$ can be `factored through' the latent space by introducing an encoder $q^\phi(z|x)$, replacing the constraint of distribution matching in the data-space ($\int \gamma(x'|x) q_X(x) dx = p_X^\theta(x')$) with distribution matching in the latent space ($\int q^\phi(z|x) q(x) dx = p(z)$).

Intuitively, $P_Z$ is a different parametrisation of $P_X^\theta$ and so if $Q_X$ pushed through the encoder results in $P_Z$, pushing $Q_X$ through the composition of the encoder and the generator will result in $P_X^\theta$. 
While it is clear that the composition of any such encoder and the generator induces a valid $\gamma$, it is non-trivial that any $\gamma$ can be decomposed as such. 
This is proved in Theorem 1 of \cite{tolstikhin2017wasserstein}.

Writing $Q_Z = \int Q_{Z|X=x} q(x) dx$ to be the latent space distribution obtained by pushing the data through the encoder, and $g$ for the deterministic generator, we obtain the following alternative statement of the optimal transport distance in the LVM setting:
%
\begin{align*}
OT_c(P_X^\theta, Q_X) = \min_{Q_{Z|X}: Q_Z = P_Z} \mathbb{E}_{x\sim Q_X} \mathbb{E}_{z\sim Q_{Z|X=x}} \left[ c(x, G(z)) \right].
\end{align*}
%
This optimisation problem is again not feasible to solve due to the hard constraint, 
but can be made computationally feasible by relaxing the constraint to obtain the WAE objective:
%
\begin{align}
L_{\text{WAE}}^{\lambda, D}(\theta, \phi) = \underbrace{\mathbb{E}_{x\sim Q_X} \mathbb{E}_{z\sim Q^\phi_{Z|X=x}} \left[ c(x, G^\theta(z)) \right]}_{\text{(i)}} + \underbrace{\lambda D\left(Q^\phi_Z, P_Z  \right)}_{\text{(ii)}}
\end{align}
%
where $D$ is some divergence, $\lambda$ is a positive scalar and the encoder is given parameter $\phi$.
For general choices of $\lambda$ and $D$, $\inf_{\phi} L_{\text{WAE}}^{\lambda, D}(\theta, \phi)$ is neither an upper nor lower bound on the original objective $OT_c(P_X^\theta, Q_X)$, but a heuristic approximation due to the relaxation of the constraint. 

%It was, however, proven by \cite{sinkhorn} (Theorem 2.1) that if $G$ is $\lambda$-Lipschitz, , then taking $D$ to be

Observe that term (i) of the WAE loss is simply a reconstruction loss, corresponding to the average distance between data sampled from $Q_X$ and their reconstruction obtained by pushing through the encoder and decoder.
This is therefore simple to estimate and hence optimise with respect to the parameters $\phi$ and $\theta$.
The term (ii) is still potentially challenging to estimate depending on the choice of $D$. 
\cite{tolstikhin2017wasserstein} propose two choices for $D$ which can be estimated based on samples from $P_Z$ and $Q_Z^\phi$: the Maximum Mean Discrepancy (MMD) \citep{gretton2012kernel} which can be estimated directly, leading to WAE-MMD, and a GAN style estimation of the Jensen-Shannon divergence by introducing an additional discriminator to obtain a lower bound on the divergence, leading to WAE-GAN. 

Estimating the discrepancy between $P_Z$ and $Q_Z^\phi$ using sample-based methods does not make use of a significant degree of structure that is present in the problem. $P_Z$ is typically chosen to be a simple distribution with known density. While $Q_Z^\phi$ may be complex, its density can be decomposed as $q^\phi(z) = \mathbb{E}_{x\sim Q_X} q^\phi(z|x)$ where $q^\phi(z|x)$ is also typically chosen to be simple (e.g. Gaussian) and $Q_X$ can be sampled.
The main contribution of this chapter, beginning in the next section, is to propose and analyse an estimator for the case that $D$ in (ii) is chosen to be an $f$-divergence. 

