%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Second Chapter *********************************
%*******************************************************************************

\chapter{Generative Modelling with Latent Variable Models}

\ifpdf
    \graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
\else
    \graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi

\emph{This chapter introduces key ideas in the literature of generative modelling with latent variable models relevant to this thesis:}
\emph{Chapter \ref{chapter:latent-space-learning-theory} presents learning theoretic results for divergence estimation in the latent spaces of autoencoders, a type of latent variable generative model;}
\emph{Chapter \ref{chapter:ica}, which concerns Independent Component Analysis (ICA), presents identifiability results for a particular class of latent variable models.}


\section{Introduction}\label{sec:generative-modelling-tour}

Suppose that a dataset of samples, drawn \iid~from some unknown data distribution $Q_X$, is given.
The high level goal of generative modelling is to learn a model distribution $P_X$ that approximates the unknown data distribution $Q_X$ based on these samples.
Latent variable models are a flexible way to specify such distributions, and work by composing simple distributions over (unobserved) latent variables with mappings to the observed data space.
The applications of latent variable generative modelling are diverse, since
having such a model of the data may be desirable for a variety of reasons, for instance: to artificially generate new samples of data \citep{goodfellow2014generative, oord2016wavenet}; to perform compression \citep{townsend2019practical, townsend2019hilloc}; to unsupervisedly learn features for transfer to other tasks \citep{tschannen2018recent, donahue2019large}; or to extract latent structure present in the data \citep{hyvarinen2000independent}.

The goal of this chapter is to introduce the necessary background and context for Chapters \ref{chapter:latent-space-learning-theory} and \ref{chapter:ica} of this thesis.
Although both concern latent variable generative models, these chapters are set in different niches of the machine learning literature.
Chapter \ref{chapter:latent-space-learning-theory} presents learning theoretic results that are relevant to the \emph{neural sampler community}, a field centred around the generation of artificial data, for which Sections \ref{sec:literature-lvms}, \ref{subsec:gen-model-divergence} and \ref{sec:literature-gen-models} of this chapter are relevant.
Chapter \ref{chapter:ica} concerns the \emph{Independent Component Analysis (ICA) community}, in which the goal is inference of latent structure, for which Sections \ref{sec:literature-lvms} and \ref{sec:literature-density-ratio-estimation} of this chapter are relevant.


%Both Chapters \ref{chapter:latent-space-learning-theory} and \ref{chapter:ica} of this thesis directly concern generative models, and the goal of this chapter is to introduce the required background.


%Generative modelling has in recent years become synonymous with the artificial generation of natural images or audio \citep{goodfellow2014generative, oord2016wavenet}.
%The mathematical formulation of this problem is, however, more general, and has close connections to other fields such as Independent Component Analysis (ICA) that will be covered in Chapter \ref{chapter:ica}.

%These correspond to (i) the community surrounding Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), mostly relevant to Chapter \ref{chapter:latent-space-learning-theory}, will be referred to as the \emph{neural sampler community} as the goal is often to produce good quality artificial samples of data; and (ii)
%the \emph{Independent Component Analysis (ICA) community}, mostly relevant to Chapter \ref{chapter:ica}, in which the goal is inference of latent structure.



% from which new samples can be drawn. 
In the neural sampler community, the problem of generative modelling is made precise with specification of a choice of divergence
%\footnote{Defined in Section \ref{subsec:gen-model-divergence}} 
$D$ and family of distributions $P_X^\theta$ with parameter $\theta\in\Theta$, the goal then being:
%
\begin{align*}
\min_{\theta \in \Theta} \ D\left(P_X^\theta,  Q_X \right)
\end{align*}
%
There are two main challenges in practically implementing and solving this problem.
First, when the data are drawn from complex high dimensional distributions, how can this be modelled with a parameterised distribution that is computationally tractable?
Second, what are appropriate choices of divergences, and how can they be estimated or minimised with respect to the parameters $\theta$?
%These questions will be discussed in the rest of this chapter. 

In the ICA community, the goal is the use the learned model $P_X^\theta$ to infer the values of latent variables. 
Since latent variables are by definition unknown, it is important that if multiple solutions to the generative modelling problem exist, they should correspond to similar models with regard to the use of the latent variables.
That is, if distinct parameters $\theta_1 \not= \theta_2$ are such that  $P_X^{\theta_1} = P_X^{\theta_2} = Q_X$, the corresponding models should be strongly related; for instance, corresponding to the same model but with permuted coordinates over the latent variables.
Such results are known as \emph{identifiability results} and are important in the theoretical study of ICA algorithms. 
ICA will be discussed in more detail in Chapter \ref{chapter:ica}, in which novel identifiability results are presented.

%One of the main families of methods for solving this problem, autoencoders, involve the introduction of a latent space and encoder, a mapping from the data to the latent space.
%The main contribution of Chapter \ref{chapter:latent-space-learning-theory} is a learning theoretic analysis of the estimation of divergences in this latent space. 


\section{Latent variable models}\label{sec:literature-lvms}

A Latent Variable Model (LVM) is a way to specify complex distributions over potentially high-dimensional spaces using simple components. 
These are flexible models that are used widely in the machine learning literature and as such, different parts of the literature often use different terminology to describe fundamentally similar ideas.
In the following, two sets of nomenclature will be introduced for the neural sampler and ICA communities.

\medskip

\begin{definition}[Latent Variable Model]
A Latent Variable Model (LVM) over a data space $\mathcal{X}$ consists of a distribution $P_Z$ over a low dimensional latent space $\mathcal{Z}$ together with conditional distributions $P_{X|Z}$. 
Together, these induce a distribution $P_X$ over the data space. 
\end{definition}

%In practice, an LVM may be parametrised, either by parametrising the prior as $P_Z^\theta$, or the conditional distributions as $P_{X|Z}^\theta$, or both.


In the neural sampler community, $P_Z$ is referred to as a \emph{prior} or \emph{noise distribution} and is usually fixed to be some simple distribution such as a unit Gaussian or uniform distribution. 
In the ICA community, $P_Z$ is referred to as a \emph{source distribution} and may be specified only implicitly through some assumed properties, such as being a factorised distribution.

The conditional distributions can be thought of as a mapping $g:\mathcal{Z}\to\mathcal{P}(\mathcal{X})$ from elements of $\mathcal{Z}$ to distributions over $\mathcal{X}$,
and may in general be Dirac delta distributions (where all probability mass is placed at a single point), in which case the associated mapping $g$ is well-defined as a function $g:\mathcal{Z}\to\mathcal{X}$. 

In the neural sampler community, the conditional distributions are referred to as \emph{generators} or \emph{decoders}, and when given parameter $\theta$ may be written as $P_{X|Z}^\theta$ or $g^\theta$. 
If the conditional distributions they correspond to are Dirac delta distributions, the generators are called \emph{deterministic}, otherwise they are \emph{stochastic}.
In the ICA community, the generators are generally deterministic and are known as \emph{mixing functions}, and are usually denoted by $f$.

When practically implemented in modern applications, generators are often realised as neural networks.
This is straightforward in the deterministic case: $g^\theta$ is simply a function and can thus be represented as a deep network with parameters $\theta$.
If the generator is stochastic, it can still be explicitly represented as a neural network provided that the conditional distributions are sufficiently structured. 
For instance, if the conditional distributions are Gaussian with varying mean and covariance, $g^\theta$ can be represented as a neural network with two outputs, one for the mean and one for the covariance.


For a fixed choice of parameter $\theta$, $P_Z$ and $P^\theta_{X|Z}$ specify a joint distribution $P^\theta_{XZ}$ over $\mathcal{X} \times \mathcal{Z}$ and thus a distribution distribution $P_X^\theta$ over the data space $\mathcal{X}$. 
For simplicity, it will be assumed that densities of all relevant distributions exist, so that this is equivalent to specifying $P_X^\theta$ via the integral
%
\begin{align*}
p^\theta(x) = \int p^\theta(x|z) p(z) dz.
\end{align*}
%
In some special cases, the density $p^\theta(x)$ may be tractable: for instance, if the generators are deterministic and invertible with known Jacobian, as is often the case in the ICA community.
In most cases in the neural sampler community, the integral is intractable, meaning that $P_X^\theta$ has unknown density in practice.
LVMs are nonetheless useful here because samples from $P_X^\theta$ can be drawn easily:
one samples first a value $z\sim P_Z$ and then $x \sim P^\theta_{X|Z=z}$. 
All relevant distributions can be chosen so that these sampling procedures are simple, e.g.\:if $P_Z$ and all $P^\theta_{X|Z}$ are Gaussian. 


\section{Divergences}\label{subsec:gen-model-divergence}

A divergence is a notion of dissimilarity between pairs of distributions that is weaker than a metric.

\medskip

\begin{definition}
A divergence $D$ is a mapping $D: \mathcal{P}(\mathcal{X}) \times \mathcal{P}(\mathcal{X}) \to \mathbb{R} \cup \{\infty\}$ such that

\begin{itemize}
\item $D(P, Q)  \geq 0$ for any distributions $P, Q \in \mathcal{P}(\mathcal{X})$,
\item $D(P, Q) = 0$ if and only if $P = Q$,
\end{itemize}

where $\mathcal{P}(\mathcal{X})$ denotes the set of all distributions on $\mathcal{X}$.
\end{definition}


A metric is additionally symmetric and obeys the triangle inequality.
For a divergence $D(P^\theta_X, Q_X)$ to be useful in the context of generative modelling, it must be possible to minimise it with respect to the parameters $\theta$. 
There are two main families of divergences that are used in the machine learning literature generally.
At a high level, Integral Probability Metrics (IPMs) can be thought of as comparing distributions by considering the difference between their densities, while $f$-divergences can be thought of as considering the ratio of their densities.
These two families are discussed in the rest of this section.

\subsection{Integral Probability Metrics}\label{subsec:intro-ipm}

\begin{definition}
An Integral Probability Metric (IPM) is a divergence that can be written as
%
\begin{align*}
D_{\mathcal{H}}(P, Q) = \sup_{h\in\mathcal{H}} \left| \int h(x) dP(x) - \int h(x) dQ(x) \right|
\end{align*}
%
for some restricted function class $\mathcal{H}: \mathcal{X} \to \mathbb{R}$. 
\end{definition}

Elements of $\mathcal{H}$ are referred to as \emph{witness functions}.
If $\mathcal{H}$ is too small, $D_\mathcal{H}$ may not be a divergence.
For instance, taking $\mathcal{H}$ to contain only the constant $0$ function results in $D_{\mathcal{H}}(P, Q) = 0$ for any $P$ and $Q$.
On the other hand, if $\mathcal{H}$ is too rich then $D_{\mathcal{H}}$ may not be useful: 
taking $\mathcal{H}$ to be the set of all real-valued measurable functions leads to the trivial (Theorem 1, \cite{sriperumbudur2009integral})
\begin{align*}
D_{\mathcal{H}}(P, Q) = \begin{cases} 0 &\text{ if } P = Q, \\ \infty &\text{ if } P \not= Q .\end{cases}
\end{align*}
%
%which would in practice not be useful since a degree of smoothness in $P$ and $Q$ is normally desirable.
Provided $\mathcal{H}$ is sufficiently rich that $D_{\mathcal{H}}$ is a divergence, it is in fact a metric, obeying the triangle inequality and symmetry.
These properties are inherited from the function $d(x,y) = |x - y|$.
%For symmetry, observe that for any $h$, 
%%
%\begin{align*}
%\left| \int h(x) dP(x) - \int h(x) dQ(x) \right| = \left| \int h(x) dQ(x) - \int h(x) dP(x) \right|,
%\end{align*}
%%
%and thus taking the supremum over $h \in \mathcal{H}$ yields $D_{\mathcal{H}}(P, Q) = D_{\mathcal{H}}(Q, P)$. 
%For the triangle inequality


Commonly encountered IPMs include \citep{sriperumbudur2009integral}:
\begin{itemize}	
\item Wasserstein or optimal transport distances, where $\mathcal{H}$ is the set of all functions with Lipschitz constant $1$ with respect to some base metric;
\item The Total Variation distance, where $\mathcal{H}$ is the set of all functions with infinity norm $1$; and
\item The Maximum Mean Discrepancy, where $\mathcal{H}$ is the set of functions in a reproducing kernel Hilbert space with norm at most $1$ induced by some kernel $k$ \citep{gretton2012kernel}.
\end{itemize}
 
%Optimal transport distances will be discussed further in Section, making use of a different formulation of these distances than as IPMs.

\subsection{$f$-divergences}\label{subsec:f-divergences-intro}

%\begin{definition}[$f$-divergence]
%\label{def:fdiv}
%Let $f$ be a convex function on $(0, \infty)$ with $f(1) = 0$. 
%The $f$-divergence $D_f$ between distributions $Q_Z$ and $P_Z$ admitting densities $q(z)$ and $p(z)$ respectively is
%{\addtolength{\abovedisplayskip}{-0.5mm}
%\addtolength{\belowdisplayskip}{-0.5mm}
%\begin{align*}
%    D_f(Q_Z \| P_Z) := \int f \left( \frac{q(z)}{p(z)} \right) p(z) dz.
%\end{align*}}%
%\end{definition}
%Many commonly used divergences such as Kullback–Leibler and $\chi^2$ are $f$-divergences.
%All the divergences considered in this paper together with their corresponding $f$ can be found in Appendix~\ref{appendix:f-fns}. 
%Of them, possibly the least well-known in the machine learning literature are $f_\beta$-divergences \cite{osterreicher2003new}. 
%These symmetric divergences are continuously parameterized by $\beta\in(0, \infty]$. Special cases include squared-Hellinger ($\mathrm{H}^2$) for ${\beta=\frac{1}{2}}$,  Jensen-Shannon (JS) for $\beta=1$, Total Variation (TV) for $\beta=\infty$. 

%\todo{flesh out this section, give more properties of $f$-divergences?}

$f$-divergences are a family of divergences that compare pairs of distributions via their density ratio, and are widespread and important in the statistics literature \citep{csiszar2004information, liese2006divergences, tsybakov2009} and are studied in Chapter \ref{chapter:latent-space-learning-theory}.

\medskip

\begin{definition}
Let $f$ be a convex real-valued function defined on $(0, \infty)$ such that $f(1)=0$, and let $P$ and $Q$ be distributions with densities $p(x)$ and $q(x)$.
The $f$-divergence between $P$ and $Q$ is defined as
%
\begin{align*}
D_f(P, Q) = \int f\left(\frac{p(x)}{q(x)}\right) q(x) dx.
\end{align*}
%
\end{definition}
This is defined for distributions $P$ and $Q$ for which $P$ is absolutely continuous with respect to $Q$, meaning informally that the density ratio $p(x)/q(x)$ is finite for all $x$ with mass under $Q$, and is usually taken to be $\infty$ otherwise.

A useful property of $f$-divergences is that for any constant $c$, replacing $f(u)$ by $\tilde{f}(u) = f(u) + c(u-1)$ does not change the divergence $D_f$:
%
\begin{align*}
D_{\tilde{f}}(P, Q) &= \int \left[ f\left(\frac{p(x)}{q(x)}\right) + c\left(\frac{p(x)}{q(x)} - 1\right) \right] q(x) dx \\
&=  \int f\left(\frac{p(x)}{q(x)}\right) q(x) dx + c \int p(x) - q(x) dx \\
&= D_f(P, Q)
\end{align*}
%
where the last equality holds since $p(x)$ and $q(x)$ integrate to $1$.
It is often convenient to work with $f_0(u) := f(u) - f'(1)(u-1)$ which is decreasing on $(0, 1)$, increasing on $(1, \infty)$ and satisfies $f'_0(1)=0$.


To see that $f$-divergences are indeed divergences, consider first non-negativity, which follows from convexity of $f$ and Jensen's inequality:
%
\begin{align}
D_f(P, Q) &= \int f\left(\frac{p(x)}{q(x)}\right) q(x) dx \nonumber \\ 
&\geq f\left(\int \frac{p(x)}{q(x)} q(x) dx\right) \label{eqn:f-divergece-jensen}\\
&= f(1) = 0. \nonumber
\end{align}
%

To show that $D_f(P,Q) = 0$ if and only if $P=Q$, note first that if $P=Q$ then $D_f(P,Q) = 0$, since $f(1)=0$.
To see that $D_f(P,Q) > 0$ for any $P \not= Q$, observe that Inequality \ref{eqn:f-divergece-jensen} above is strict for any $f$ that is strictly convex.
%
This also holds for any $f$ that does not have constant gradient in any neighbourhood of $1$, since the $f_0$ associated to any such $f$ is strictly positive on $\mathbb{R}_+\setminus \{1\}$.
For any $P\not= Q$, the distribution $Q$ must put positive mass in areas for which $p(x)/q(x) \not= 1$ and so  $D_{f_0}(P,Q)$ must be positive.
It follows that $D_f = D_{f_0}$ is also positive.
Hence `most' choices of $f$ lead to valid $f$-divergences.


Different choices of $f$ yield several commonly encountered divergences, including the Kullback-Leibler, Jensen-Shannon, Total Variation, $\chi^2$ and $\alpha$-divergences as well as the lesser known $\beta$-divergences \citep{osterreicher2003new}.
This family of symmetric divergences are continuously parameterized by $\beta\in(0, \infty]$ and include as special cases the squared-Hellinger $({\beta=\frac{1}{2}})$,  Jensen-Shannon $(\beta=1)$ and Total Variation $(\beta=\infty)$. 
Table \ref{table:f-fns} lists the $f_0$ and other commonly encountered choices of $f$ for the divergences considered in Chapter \ref{chapter:latent-space-learning-theory}.

%This is similarly the case for any $f$ for which there exists some $c>0$ such that $f(u) > c(u-1)$ for all $u\not=1$. 
%
%Indeed, for any $P \not= Q$ there exists a set $A \subset \mathcal{X}$ with positive measure under $Q$ on which $p(x)/q(x) \not= 1$ (if this were not the case, we would have $p(x)/q(x) = 1$ everywhere that $Q$ puts mass and so $P=Q$).
%Thus,
%\begin{align*}
%D_f(P, Q) &= \int_A f\left(\frac{p(x)}{q(x)}\right) q(x) dx +  \int_{\mathcal{X} \setminus A} f\left(\frac{p(x)}{q(x)}\right) q(x) dx\\ 
%&> \int_A c \left(\frac{p(x)}{q(x)} - 1\right) q(x) dx +  \int_{\mathcal{X} \setminus A} c \left(\frac{p(x)}{q(x)} - 1\right) q(x) dx\\ 
%&= c > 0.
%\end{align*}





%\section{$f$ for divergences considered in this paper}\label{appendix:f-fns}



{
\renewcommand{\arraystretch}{2}
\begin{table}
 \caption{$f$ and $f_0$ of divergences referenced in this thesis.}
 \label{table:f-fns}
 \centering
 \begin{tabular}{c c c} 
 \toprule
 $f$-divergence & $f_0(x)$ & Other typical $f(x)$ \\
 \midrule
 Kullback-Leibler (KL) & $x \log x - x + 1$ & $x \log x$\\
 Total Variation (TV) & $\frac{1}{2}|1-x|$& - \\
 $\chi^2$ & $x^2 - 2x$& $(x-1)^2$,\: $x^2 -1$\\
 Squared-Hellinger ($\text{H}^2$) & $2(1-\sqrt{x})$&  $(\sqrt{x} - 1)^2$\\
 Jensen-Shannon (JS) & $(1+x)\log(\frac{2}{1+x}) + x\log x$& -\\
 $\alpha$-divergence, \,
% $D_{f_\alpha}$, 
 $-1<\alpha < 1$ & $\frac{4}{1-\alpha^2}\left( 1 - x^{\frac{1+\alpha}{2}} \right) - \frac{2(x-1)}{\alpha-1}$ & $\frac{4}{1-\alpha^2}\left( 1 - x^{\frac{1+\alpha}{2}} \right)$\\
 $\beta$-divergence, \,
% $D_{f_\beta}$, 
 $\beta > 0,$ $\beta\not=\frac{1}{2}$ & $\frac{1}{1-\frac{1}{\beta}}\left[ (1+x^\beta)^{\frac{1}{\beta}} - 2^{\frac{1}{\beta}-1}(1+x) \right]$& -\\
 \bottomrule
\end{tabular}
\end{table}
}


%\begin{align*}
%D_f(P, Q) = 
%    \begin{cases}
%       \int f\left(\frac{dQ}{dP}(x) \right) dP, & \text{if $dQ/dP$ exists} \\
%      \infty, & \text{otherwise}
%    \end{cases}
%\end{align*}

%Intuitively, such divergences measure the discrepancy between the two distributions by seeking functions which are maximal on one distribution and minimal on the other. 



\section{Density Ratio Estimation}\label{sec:literature-density-ratio-estimation}

A natural way to distinguish between two distributions $P$ and $Q$ with overlapping support is to consider the problem of classifying between draws from each distribution.
Suppose that samples from $P$ are labelled as class $1$, and samples from $Q$ as class $0$. 
This classification problem can be implemented as logistic regression, in which case a function $c:\mathcal{X}\to [0,1]$ is introduced and trained to minimise the objective 
%
\begin{align*}
L(c) &=  \mathbb{E}_{x\sim P}\left[ - \log c(x) \right] + \mathbb{E}_{x \sim Q} \left[ - \log(1 - c(x)) \right] \\
&= \int -\log c(x) p(x) - \log (1 - c(x)) q(x) dx
\end{align*}
%
For any particular $x$, the integrand is minimised by $c^*(x) = \frac{p(x)}{q(x) + p(x)}$ \citep{goodfellow2014generative}, and so the optimal classifier assigns $x$ to class $1$ with the posterior probability that it was drawn from $P$.
If $c$ parametrised as $\frac{1}{1+\exp( -r(x))}$ where $r:\mathcal{X}\to\mathbb{R}$, then the optimal  $c^*$ corresponds to $r^*(x) = \log\left(p(x) / q(x)\right)$.

This is a useful trick for cases in which only samples from two distributions are available and estimation of their density ratio is desired, as is the case in Chapter \ref{chapter:ica}.
Moreover, solving this classification problem is closely related to divergence estimation, since for any choice of classifier $c$,
%
\begin{align*}
L(c) \geq \log 4 - 2 \cdot D_{\text{JS}}(P, Q).
\end{align*}
%
This follows straightforwardly from the definition of the Jensen-Shannon divergence and the fact that equality is attained by the optimal $c^*$, see \cite{goodfellow2014generative} for details.
Rearranging, it follows that the Jensen-Shannon divergence between two distributions can be estimated by maximising the lower bound
%
\begin{align*}
D_{\text{JS}}(P, Q) \geq \log 2 - \frac{L(c)}{2}.
\end{align*}

Recall that the Jensen-Shannon divergence is an $f$-divergence. 
It can be similarly shown that any $f$-divergence between two distributions can be estimated by maximisation of a lower bound corresponding to a classification problem, and that doing so results in a function of the density ratio being estimated.
All $f$-divergences admit a variational form as a result of convex conjugacy \citep{nguyen10ratio}.
Any convex function $f(u)$ has a conjugate $f^*(t)$ defined as
%
\begin{align*}
f^*(t) = \sup_{u \in \dom(f)} \{ut - f(u)\}.
\end{align*}
%
The resulting function $f^*$ is itself convex, and
provided that $f$ is continuous, $f$ and $f^*$ are dual in the sense that $f^{**} = f$.
This means that $f$ can be written as
%
\begin{align*}
f(u) = \sup_{t \in \dom(f^*)} \{ut - f^*(t)\}.
\end{align*}
%
Plugging this into the definition of $f$-divergences yields
%
\begin{align*}
D_f(P, Q) &= \int q(x) \sup_{t \in \dom(f^*)} \left\{t \frac{p(x)}{q(x)} - f^*(t) \right\} dx \\
& \geq \sup_{T \in \mathcal{T}} \left\{ \int p(x) T(x) dx - \int q(x) f^*(T(x)) \right\} dx \\
&= \sup_{T \in \mathcal{T}} \mathbb{E}_{x \sim P} \left[ T(x) \right] - \mathbb{E}_{x \sim Q} \left[ f^*(T(x)) \right] 
\end{align*}
where $\mathcal{T}$ is an arbitrary class of functions $\mathcal{X} \to \dom(f^*) \subseteq \mathbb{R}$.
It can be shown \citep{nguyen10ratio} that the optimal $T^*$ attaining the supremum satisfies 
\begin{align*}
T^*(x) = f'\left( \frac{p(x)}{q(x)}\right)
\end{align*}
subject to mild conditions on $f$. 
In this sense, $T^*$ estimates (a function of) the density ratio $p(x)/q(x)$.

%The particular case of the Jensen-Shannon divergence merits further discussion for two reasons.
%First, it is relevant to the explanation of Generative Adversarial Networks in the next section. 
%Second, because it is used in Chapter \ref{chapter:ica}.




%In this chapter we do not make use of this variational formulation of $f$-divergences, but it will be used in explaining Generative Adversarial Networks in the next section.


\section{Examples of generative models}\label{sec:literature-gen-models}

Solving the generative modelling problem as posed in the introduction requires the minimisation of $D(P^\theta_X, Q_X)$ with respect to the parameters $\theta$ of the LVM in a computationally tractable way.
In general it is infeasible to estimate $D(P^\theta_X, Q_X)$ directly, or even to compute gradients of it with respect to $\theta$. 
%Thus, one must resort to approximations.


There are two main families of methods that introduce auxiliary functions as computational tricks to bound or estimate $D(P^\theta_X, Q_X)$ in a computationally tractable way.
These are \emph{Generative Adversarial Networks (GANs)}, which introduce a discriminator $d:\mathcal{X} \to [0,1]$, and \emph{autoencoders}, which introduce an encoder $e:\mathcal{X} \to \mathcal{Z}$.

The remainder of this section discusses GANs and two types of autoencoders, Variational Autoencoders (VAEs) and Wasserstein Autoencoders (WAEs), showing how specific choices of divergences can be approximated.

\subsection{Generative Adversarial Networks}

Generative Adversarial Networks (GANs) are a family of methods that approximately minimise any $f$-divergence and some choices of IPMs. 
In the case of $f$-divergences, the key ideas have been already introduced in Section \ref{sec:literature-density-ratio-estimation}.

\cite{goodfellow2014generative} introduced the idea of training a discriminator $d^\phi: \mathcal{X} \to [0,1]$ to classify between `real' samples from $Q_X$ and `fake' samples from $P^\theta_X$, which then provides a surrogate loss for a generator $g^\theta$, trained simultaneously to maximise the loss of the discriminator. 
In \cite{goodfellow2014generative}, the loss is implemented as logistic regression
%
\begin{align*}
L(\theta, \phi) = \mathbb{E}_{x\sim Q_X}\left[ -\log d^\phi(x) \right] \mathbb{E}_{x \sim P_Z} \left[- \log(1 - d^\phi(g^\theta(z)) \right]
\end{align*}
%
which, as previously discussed in Section \ref{sec:literature-density-ratio-estimation}, is a lower bound on the Jensen-Shannon divergence $D_{\text{JS}}(P^\theta_X, Q_X)$, up to constants and scalar factors.

Stochastic gradients of this loss can be taken with respect to both $\phi$ and $\theta$ by using minibatch samples of data to approximate the outer expectations when $d^\phi$ and $g^\theta$ are both neural networks.
%It was shown that this loss satisfies $L(\theta, \phi) \leq 2 \cdot D_{\text{JS}}(P^\theta_X, Q_X) - \log 4$ for any $d^\phi$ and $g^\theta$, with equality attainable only if the class of $d^\phi$ is sufficiently rich, where $D_{\text{JS}}$ is the Jensen-Shannon divergence.
Thus, although actually computing $D_{\text{JS}}(P^\theta_X, Q_X)$ is intractable, it can be approximately minimised with respect to $\theta$ by maximising the surrogate loss $L(\theta, \phi)$ with respect to $\phi$ and minimising it with respect to $\theta$.
%is (up to constants) a lower bound on this.
%Hence the Jensen-Shannon divergence can be approximately minimised by \emph{maximising} $L(\theta, \phi)$ with respect to $\phi$ and minimising this with respect to $\theta$.


\cite{nowozin2016f}, building on the work of \cite{nguyen10ratio}, generalised this to arbitrary $f$-divergences using the variational formulation of $f$-divergences to yield the surrogate loss
%
%\begin{align*}
%D_f(P, Q) &\geq \sup_{T \in \mathcal{T}} \mathbb{E}_{x \sim P} \left[ T(x) \right] - \mathbb{E}_{x \sim Q} \left[ f^*(T(x)) \right] 
%\end{align*}
%%
%where the set of functions $\mathcal{T}$ is implemented as a neural network.
%Thus, parameterising $T$ by $\phi$ yields the surrogate loss
%
\begin{align*}
L_f(\phi, \theta) = \mathbb{E}_{x \sim Q_X} \left[ T^\phi(x) \right] - \mathbb{E}_{z \sim P_Z} \left[ f^*(T^\phi(g^\theta(x))) \right] \leq D_f(P^\theta_X, Q_X)
\end{align*}
%
where the function $T^\phi$ is implemented as a neural network.
As with the original GAN objective, stochastic gradients of this surrogate loss can be computed. 
The function $T^\phi$ plays the role of the discriminator introduced by \cite{goodfellow2014generative}. %but the above derivation holds for arbitrary choices of $f$. 
\cite{nowozin2016f} thus demonstrated how the GAN `trick' can be applied to other $f$-divergences to approximately minimise $D_f(P^\theta_X, Q_X)$ with respect to the LVM parameters $\theta$.

A similar idea can also be used to approximately minimise IPMs $D_\mathcal{H}(P^\theta_X, Q_X)$, provided that the function class $\mathcal{H}$ can be practically parameterised.
If $\mathcal{H}$ is such that $h \in \mathcal{H}$ implies that $-h \in \mathcal{H}$, $D_\mathcal{H}$ can be written without the inner absolute function, leading to
%
\begin{align*}
D_{\mathcal{H}}(P, Q) &= \sup_{h\in\mathcal{H}} \int h(x) dP(x) - \int h(x) dQ(x) \\
&= \sup_{h\in\mathcal{H}} \left\{ \mathbb{E}_{x \sim P} \left[ h(x) \right]- \mathbb{E}_{x \sim Q} \left[ h(x) \right] \right\} \\
&\geq  \mathbb{E}_{x \sim P} \left[ h(x) \right]- \mathbb{E}_{x \sim Q} \left[ h(x) \right] \\
\end{align*}
%
where the inequality holds for any $h \in \mathcal{H}$, where $h$ now plays the role of the discriminator. 
If the set $\mathcal{H}$ can be differentiably parametrised, stochastic gradients of the loss can be obtained with respect its parameters.
In contrast to the $f$-divergence case, the discriminator $h$ must belong to a particular class of functions, which can complicate specifying the parametrisation.
One example of an IPM-based GAN is the Wasserstein GAN of \cite{AB17}. 
Here, the 1-Wasserstein distance is used, corresponding to $h$ having Lipschitz constant at most $1$. 
For certain neural network architectures, including those composed of fully-connected and convolutional layers, this can be enforced by weight clipping.

\subsection{Variational Autoencoders}

Variational Autoencdoers (VAEs) \citep{kingma2013auto, rezende2014stochastic} are a method to minimise the KL-divergence between model and data distributions, defined as
%
\begin{align*}
D_{\text{KL}}(Q_X, P^\theta_X) &= \int q(x) \log \left( \frac{q(x)}{p^\theta(x)} \right) dx \\
&= \int q(x) \log q(x) - \int q(x) \log p^\theta(x) dx.
\end{align*}
%
The first term above, the negative of the \emph{differential entropy} of $Q_X$, often written $H(Q_X)$, cannot be estimated without knowledge of the density $q(x)$.
However, since it is constant as a function of $\theta$, it can be ignored.
The term $\log p^\theta(x)$ inside the second integral is known as the \emph{log-likelihood} or \emph{evidence}, and maximisation of this quantity is known as \emph{maximum likelihood estimation}. 
Although the density $p^\theta(x)$ is intractable, the evidence can be tractably lower bounded leading to the so-called \emph{evidence lower bound} (ELBO), which in turn leads to a tractable \emph{upper} bound on $D_{\text{KL}}(Q_X, P^\theta_X)$. 

First, observe that the log-likelihood can be written 
%
\begin{align*}
\log p^\theta(x) &= \log \left( \int p^\theta(x|z) p(z) dz \right).
\end{align*}
%
Given any distribution $q^\phi(z|x)$ depending on parameter $\phi$ and the value of $x$, we can multiply and divide inside the integral, leaving its value unchanged.
This leads to
%
\begin{align*}
\log p^\theta(x) &= \log \left( \int p^\theta(x|z) \frac{p(z)}{q^\phi(z|x)} q^\phi(z|x) dz \right) \\
&= \log \left( \mathbb{E}_{q^\phi(z|x)} \left[  p^\theta(x|z) \frac{p(z)}{q^\phi(z|x)} \right]\right) \\
&\geq \mathbb{E}_{q^\phi(z|x)} \left[ \log p^\theta(x|z) + \log p(z) - \log q^\phi(z|x) \right] \\
&= \mathbb{E}_{q^\phi(z|x)} \log p^\theta(x|z)  - D_{\text{KL}}\left(q^\phi(z|x), p(z)\right),
\end{align*}
%
where the inequality follows from Jensen's inequality due to the concavity of $\log$.
The distribution $q^\phi(z|x)$ is a variational approximation to the true posterior $p^\theta(z|x)$; it can be shown that the gap introduced by Jensen's inequality is equal to $D_{\text{KL}}\left(q^\phi(z|x), p^\theta(z|x)\right)$. 
$q^\phi(z|x)$ is often referred to as an encoder as it maps elements of the data space $\mathcal{X}$ to distributions over the latent space $\mathcal{Z}$.
Putting things together yields
%
\begin{align*}
D_{\text{KL}}(Q_X, P^\theta_X) \leq -H(Q_X)  - \mathbb{E}_{q(x)} \mathbb{E}_{q^\phi(z|x)} \log p^\theta(x|z) + \mathbb{E}_{q(x)}D_{\text{KL}}\left(q^\phi(z|x), p(z)\right)
\end{align*}
%
where $H(Q_X)$ is the constant differential entropy, leading to the VAE loss
%
\begin{align}
L_{\text{VAE}}(\theta, \phi) =  \underbrace{\mathbb{E}_{q(x)} \mathbb{E}_{q^\phi(z|x)} \left[ - \log p^\theta(x|z)\right]}_{\text{(i)}} + \underbrace{\mathbb{E}_{q(x)}D_{\text{KL}}\left(q^\phi(z|x), p(z)\right)}_{\text{(ii)}}
\end{align}
%
which (up to constants) is an upper bound on $D_{\text{KL}}(Q_X, P^\theta_X)$.
It is common for the prior to be a standard Gaussian, the generator to output Gaussians with mean $\mu^\theta(z)$ and fixed isotropic covariance, and the encoder to map to Gaussians with mean $\mu^\phi(x)$ and diagonal covariance $\Sigma^\phi(x)$.
In this case, term (i) above can be interpreted as an average reconstruction loss and (ii) as a regulariser, hence making the model a type of regularised autoencoder\footnote{Note however that the interpretation of VAEs as a type of autoencoder breaks down when more powerful classes of generators are used, as discussed in \href{http://paulrubenstein.co.uk/variational-autoencoders-are-not-autoencoders/}{http://paulrubenstein.co.uk/variational-autoencoders-are-not-autoencoders/}.}.

The encoder $Q^\phi_{Z|X}$ together with the data distribution $Q_X$ induce the push-forward distribution $Q^\phi_Z$ known as the \emph{aggregate posterior}.
The term (ii) was shown by \cite{hoffman2016elbo} to be equivalent to $D_{\text{KL}}(Q^\phi_Z , P_Z) + I(X,Z)$ where $I(X,Z) = D_{\text{KL}}(Q^\phi_{XZ} , Q_XQ^\phi_{Z})$ is the mutual information of a data sample and its encoding.
Chapter \ref{chapter:latent-space-learning-theory} concerns estimation of $f$-divergences (and hence the KL-divergence) between priors and aggregate posteriors, and so is directly relevant to mutual information estimation via this equivalence.

%All distributions involved can be chosen so this can be unbiasedly estimated, meaning that it can be minimised with respect to the parameters $\theta$ and $\phi$ using stochastic gradient methods.
%It is common that the encoder, generator and prior are chosen to be Gaussians, 



%\todo{discuss the fact that the aggregate posterior and prior satisfy the setting we consider and mention that some works consider divergences between them?}

\subsection{Wasserstein Autoencoders}

Wasserstein Autoencoders (WAEs) \citep{tolstikhin2017wasserstein} approximately minimise optimal transport distances, also known as 1-Wasserstein distances, between model and data distributions.
These were discussed briefly in Section \ref{subsec:intro-ipm} as they can be expressed as IPMs, but here an alternative formulation of these distances will be used.

Let $c$ be any metric on $\mathcal{X}$.
For intuition, $c(x, x')$ may be thought of as a function specifying the cost of transporting a point from $x$ to $x'$.
The optimal transport distance between two distributions $P$ and $Q$ over $\mathcal{X}$ is then the minimal cost incurred by transporting the probability mass of $P$ to that of $Q$.
%For simplicity in the following, we will assume $P$ and $Q$ have densities $p$ and $q$. 

Formally, let $\Gamma$ be the set of joint distributions over $\mathcal{X} \times \mathcal{X}$ with marginals $P$ and $Q$. 
That is, any element $\gamma(x, x') \in \Gamma$ is a joint distribution satisfying $\gamma(x) = p(x)$ and $\gamma(x') = q(x')$.
Then, the optimal transport distance is defined as
%
\begin{align*}
OT_c(P, Q) = \min_{\gamma \in \Gamma} \mathbb{E}_{x, x' \sim \gamma} \left[ c(x, x') \right].
\end{align*}
%
This can equivalently be written as
%
\begin{align*}
OT_c(P, Q) = \min_{\gamma \in \Gamma} \mathbb{E}_{x\sim Q} \mathbb{E}_{x'\sim \gamma(x'|x)} \left[ c(x, x') \right],
\end{align*}
%
where $\gamma$ can be thought of as a conditional distribution specifying how an element of probability mass at $x$ should be moved and spread over the points $x'$. 
Specified this way, each element of $\Gamma$ should satisfy $\int \gamma(x'|x) q(x) dx = p(x')$.
In the generative modelling setting, we thus have
%
\begin{align*}
OT_c(P_X^\theta, Q_X) = \min_{\gamma \in \Gamma} \mathbb{E}_{x\sim Q_X} \mathbb{E}_{x'\sim \gamma(x'|x)} \left[ c(x, x') \right].
\end{align*}
%
In practice, this minimisation problem cannot be solved directly: given a candidate conditional distribution $\gamma(x'|x)$ it is not practically possible to verify whether or not $\int \gamma(x'|x) q_X(x) dx = p_X^\theta(x')$ since only samples from $Q_X$ are available and the density $p^\theta_X(x)$ is intractable.

\cite{tolstikhin2017wasserstein} proved the following result, giving a handle on this problem:
If the generator $p^\theta(x|z)$ is deterministic, any valid $\gamma$ can be written as a composition $\gamma(x'|x) = \int p^\theta(x'|z) q^\phi(z|x) dz$ where $q^\phi(z|x)$ is a conditional distribution satisfying $\int q^\phi(z|x) q(x) dx = p(z)$. 
That is, any $\gamma$ can be `factored through' the latent space by introducing an encoder $q^\phi(z|x)$, replacing the constraint of distribution matching in the data-space ($\int \gamma(x'|x) q_X(x) dx = p_X^\theta(x')$) with distribution matching in the latent space ($\int q^\phi(z|x) q(x) dx = p(z)$).

Intuitively, $P_Z$ is a different parametrisation of $P_X^\theta$ and so if $Q_X$ pushed through the encoder results in $P_Z$, pushing $Q_X$ through the composition of the encoder and generator will result in $P_X^\theta$. 
While it is clear that the composition of any such encoder and the generator induces a valid $\gamma$, it is non-trivial that any $\gamma$ can be decomposed as such. 
This is proved in Theorem 1 of \cite{tolstikhin2017wasserstein}.

Writing $Q_Z = \int Q_{Z|X=x} q(x) dx$ to be the latent space distribution obtained by pushing the data through the encoder and $g$ for the deterministic generator yields the following alternative statement of the optimal transport distance in the LVM setting:
%
\begin{align*}
OT_c(P_X^\theta, Q_X) = \min_{Q_{Z|X}: Q_Z = P_Z} \mathbb{E}_{x\sim Q_X} \mathbb{E}_{z\sim Q_{Z|X=x}} \left[ c(x, G(z)) \right].
\end{align*}
%
Again this optimisation problem is not feasible to solve due to the hard constraint, 
but it can be made computationally feasible by relaxing the constraint to obtain the WAE objective:
%
\begin{align}
L_{\text{WAE}}^{\lambda, D}(\theta, \phi) = \underbrace{\mathbb{E}_{x\sim Q_X} \mathbb{E}_{z\sim Q^\phi_{Z|X=x}} \left[ c(x, G^\theta(z)) \right]}_{\text{(i)}} + \underbrace{\lambda D\left(Q^\phi_Z, P_Z  \right)}_{\text{(ii)}}
\end{align}
%
where $D$ is some divergence, $\lambda$ is a positive scalar and the encoder is given parameter $\phi$.
While the generator is required to be deterministic, the encoder may be deterministic or stochastic \citep{rubenstein2018latent}.
For general choices of $\lambda$ and $D$, $\inf_{\phi} L_{\text{WAE}}^{\lambda, D}(\theta, \phi)$ is neither an upper nor lower bound on the original objective $OT_c(P_X^\theta, Q_X)$, but a heuristic approximation due to the relaxation of the constraint. 

%It was, however, proven by \cite{sinkhorn} (Theorem 2.1) that if $G$ is $\lambda$-Lipschitz, , then taking $D$ to be

Term (i) of the WAE loss is simply a reconstruction loss, corresponding to the average distance between data sampled from $Q_X$ and their reconstruction obtained by pushing through the encoder and decoder.
This is therefore simple to estimate and optimise with respect to the parameters $\phi$ and $\theta$.
Term (ii) is potentially challenging to estimate depending on the choice of $D$. 
\cite{tolstikhin2017wasserstein} propose two choices for $D$ which can be estimated based on samples from $P_Z$ and $Q_Z^\phi$: the Maximum Mean Discrepancy (MMD) \citep{gretton2012kernel} which can be estimated directly, leading to WAE-MMD, and a GAN style estimation of the Jensen-Shannon divergence by introducing an additional discriminator to obtain a lower bound on the divergence, leading to WAE-GAN. 

Estimating a divergence between $P_Z$ and $Q_Z^\phi$ using sample-based methods does not make use of a significant degree of structure that is present in the problem. $P_Z$ is typically chosen to be a simple distribution with known density. While $Q_Z^\phi$ may be complex, its density can be decomposed as $q^\phi(z) = \mathbb{E}_{x\sim Q_X} q^\phi(z|x)$ where $q^\phi(z|x)$ is also typically chosen to be simple (e.g. Gaussian) and $Q_X$ can be sampled.
The main contribution of Chapter \ref{chapter:latent-space-learning-theory} is to propose and analyse an estimator making use of this structure for the case that $D$ in (ii) is chosen to be an $f$-divergence. 

