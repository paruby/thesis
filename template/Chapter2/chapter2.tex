%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Second Chapter *********************************
%*******************************************************************************

\chapter{Literature review}

\ifpdf
    \graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
\else
    \graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi

In this chapter we review the literature relevant to the thesis.

\begin{itemize}
\item The terms \emph{representation} and \emph{representation learning} in machine learning are somewhat ill-defined, with their precise meaning depending on the particular context or niche of research under consideration. Common to these subtly different meanings is the existence of a data space $\mathcal{X}$ of raw data and a function or \emph{feature extractor} $f:\mathcal{X} \to \mathcal{Z}$ mapping to another space $\mathcal{Z}$. In many cases, $\mathcal{X}$ may be high dimensional natural data such as image or audio, while $\mathcal{Z}$ is a lower dimensional vector space.
\item This is a thesis about representations in a general sense, covering three different sub-fields of modern machine learning research. As such, in this literature review it is necessary to cover a lot of ground. The literature review begins with a broad overview of representation learning and the different ways that representations arise across machine learning.
\item In the major part of the literature review, we cover the background of each of the three areas separately in detail.
\item We give an introduction to causality and causal inference to set the stage of Chapter 3.
\item ICA is covered next in order to introduce Chapter 4.
\item Finally, we discuss generative modelling in order to set up Chapters 5 and 6.
\item Each of the above named chapters additionally begins with a smaller literature review that is more focussed to the particular problem setting considered.
\end{itemize}