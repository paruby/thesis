%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Third Chapter **********************************
%*******************************************************************************
\chapter{Latent space learning theory}\label{chapter:latent-space-learning-theory}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter5/Figs/Raster/}{Chapter5/Figs/PDF/}{Chapter5/Figs/}}
\else
    \graphicspath{{Chapter5/Figs/Vector/}{Chapter5/Figs/}}
\fi

\emph{This chapter is largely based on the paper:}

\begin{quote}
\fullcite{rubenstein2019practical}
\end{quote}

\emph{Additionally, the following related workshop papers were published during my PhD but are not included in this thesis:}

\begin{quote}
\fullcite{rubenstein2018wasserstein}
\end{quote}

\begin{quote}
\fullcite{rubenstein2018learning}
\end{quote}

\emph{The main contribution of this chapter is a learning theoretic analysis of estimating divergences between distributions. 
The setting considered by this work occurs naturally in and has application to the latent variable models used in the generative modelling community, in particular Wasserstein Autoencoders.
Sections ?? - ?? are a review of generative modelling, setting the stage for Sections ?? - ?? which is based on} \cite{rubenstein2019practical}.

\todo{replace $D(.\|.)$ with $D(.,.)$}

\section{Introduction}

The estimation and minimization of divergences between probability distributions based on samples are fundamental problems of machine learning.
For example, maximum likelihood learning can be viewed as minimizing the Kullback-Leibler divergence $D_{\text{KL}}(P_{\text{data}}, P_{\text{model}})$ with respect to the model parameters.
More generally, generative modelling
%---most famously Variational Autoencoders and Generative Adversarial Networks \cite{kingma2013auto, goodfellow2014generative} discussed in the next section---
can be viewed as minimizing a divergence $\smash{D(P_{\text{data}}, P_{\text{model}})}$ where $\smash{P_{\text{model}}}$ may be intractable.
In variational inference, an intractable posterior $p(z|x)$ is approximated with a tractable distribution $q(z)$ chosen to minimize $\smash{D_{\text{KL}}\bigl(q(z) , p(z|x)\bigr)}$.
The mutual information between two variables $\smash{I(X,Y)}$, core to information theory and Bayesian machine learning, is equivalent to $\smash{D_{\text{KL}}(P_{X,Y} , P_X P_Y)}$. 
Independence testing often involves estimating a divergence $\smash{D(P_{X,Y} , P_X P_Y)}$, while two-sample testing (does $P=Q$?) involves estimating a divergence $D(P,Q)$.
Additionally, one approach to domain adaptation, in which a classifier is learned on a distribution $P$ but tested on a distinct distribution $Q$, involves learning a feature map $\phi$ such that a divergence $\smash{D\left( \phi_\# P , \phi_\# Q \right)}$ is minimized, where $\smash{\phi_\#}$ represents the push-forward operation \cite{ben2007analysis,ganin2016domain}.

This chapter considers the well-known family of $f$-divergences \cite{csiszar2004information, liese2006divergences} that includes amongst others the $\KL$, Jensen-Shannon ($\JS$), $\chi^2$, and $\alpha$-divergences as well as the Total Variation ($\TV$) and squared Hellinger ($\Hsq$) distances, the latter two of which play an important role in the statistics literature \cite{tsybakov2009}.
A significant body of work exists studying the estimation of the $f$-divergence $D_f(Q , P)$ between general probability distributions $Q$ and $P$.
While the majority of this focuses on $\alpha$-divergences and closely related R\'enyi-$\alpha$ divergences \citep{poczos11alpha, singh14alpha, krishnamurthy14icml},
many works address specifically the KL-divergence \citep{perez08kl, wang09kl}
with fewer considering $f$-divergences in full generality \cite{nguyen10ratio, kanamori12ratio, moon14ensemble, moon14followup}.
Although the $\KL$-divergence is the most frequently encountered $f$-divergence in the machine learning literature, in recent years there has been a growing interest in other $f$-divergences \cite{nowozin2016f}, 
in particular in the variational inference community where they have been employed to derive alternative evidence lower bounds \cite{pmlr-v80-chen18k, li2016renyi, dieng2017variational}.


The main challenge in computing $D_f(Q , P)$ is that it requires knowledge of either the densities of both $Q$ and $P$, or the density ratio $dQ/dP$.
In studying this problem, assumptions of differing strength can be made about $P$ and $Q$. 
In the weakest \emph{agnostic} setting, one may be given only a finite number of i.i.d\:samples from the distributions without any further knowledge about their densities.
As an example of stronger assumptions,
both distributions may be mixtures of Gaussians \cite{hershey2007approximating, durrieu2012lower}, 
or one may have access to samples from $Q$ and have full knowledge of $P$ \citep{heroma2001techrep, heroma2002ieee} as in e.g.\:model fitting.

Most of the literature on $f$-divergence estimation considers the weaker agnostic setting.
The lack of assumptions makes such work widely applicable, but comes at the cost of needing to work around estimation of either the densities of $P$ and $Q$ \cite{singh14alpha, krishnamurthy14icml} or the density ratio $dQ/dP$ \citep{nguyen10ratio, kanamori12ratio} from samples.
Both of these estimation problems are provably hard \citep{tsybakov2009, nguyen10ratio} and suffer rates---the speed at which the error of an estimator decays as a function of the number of samples $N$---of order $\smash{N^{-1/d}}$ when $P$ and $Q$ are defined over $\R^d$ unless their densities are sufficiently smooth.
This is a manifestation of the \emph{curse of dimensionality} and rates of this type are often called \emph{nonparametric}.
One could hope to estimate $D_f(P,Q)$ without explicitly estimating the densities or their ratio and thus avoid suffering nonparametric rates, however a lower bound of the same order $\smash{N^{-1/d}}$ was recently proved for $\alpha$-divergences \citep{krishnamurthy14icml}, a sub-family of $f$-divergences.
While some works considering the agnostic setting provide rates for the bias and variance of the proposed estimator \cite{nguyen10ratio, krishnamurthy14icml} or even exponential tail bounds \citep{singh14alpha},
it is more common to only show that the estimators are asymptotically unbiased or consistent without proving specific rates of convergence \cite{wang09kl, poczos11alpha, kanamori12ratio}.


Motivated by recent advances in machine learning, this chapter studies a setting in which much stronger structural assumptions are made about the distributions.
Let $\X$ and $\Z$ be two finite dimensional Euclidean spaces.
We consider estimation of the divergence $D_f(Q_Z, P_Z)$ between two probability distributions $P_Z$ and $Q_Z$, both defined over $\Z$.
$P_Z$ has known density $p(z)$, while $Q_Z$ with density  $q(z)$ admits the factorization $\smash{q(z) := \int_\X q(z|x)q(x) dx}$ where access to independent samples from the distribution $Q_X$ with unknown density $q(x)$ and full knowledge of the conditional distribution $\smash{Q_{Z|X}}$ with density $q(z|x)$ are assumed.
In most cases $Q_Z$ is intractable due to the integral and so is $D_f(Q_Z , P_Z)$.
As a concrete example, these assumptions are often satisfied in applications of modern unsupervised generative modeling with deep autoencoder architectures,
where $\X$ and $\Z$ would be \emph{data} and \emph{latent} spaces, $\smash{P_Z}$ the \emph{prior}, $\smash{Q_X}$ the \emph{data distribution}, $\smash{Q_{Z|X}}$ the \emph{encoder}, and $\smash{Q_Z}$ the \emph{aggregate posterior}.

Given independent observations $\smash{X_1, \ldots, X_N}$ from $\smash{Q_X}$, the finite mixture $\smash{\hat{Q}_Z^N := \frac{1}{N} \sum_{i=1}^N Q_{Z|X_i}}$ can be used to approximate the continuous mixture $\smash{Q_Z}$. 
The main contribution of this chapter is to approximate the intractable $\smash{D_f(Q_Z , P_Z)}$ with $\smash{D_f(\hat{Q}_Z^N , P_Z)}$, a quantity that can be estimated to arbitrary precision using Monte-Carlo sampling since both distributions have known densities, and to theoretically study conditions under which this approximation is reasonable.
$\smash{D_f(\hat{Q}_Z^N , P_Z)}$ is denoted the Random Mixture (RAM) estimator and rates at which it converges to $\smash{D_f(Q_Z , P_Z)}$ as $N$ grows are derived.
Similar guarantees are also provided for RAM-MC---a practical Monte-Carlo based version of RAM.
By side-stepping the need to perform density estimation, one obtains \emph{parametric} rates of order $N^{-\gamma}$, where $\gamma$ is independent of the dimension (see Tables \ref{table:convergence} and \ref{table:concentration}), although the constants may still in general show exponential dependence on dimension.
This is in contrast to the agnostic setting where \emph{both} nonparametric rates and constants are exponential in dimension. 

These results have immediate implications to existing literature.
For the particular case of the $\KL$ divergence, a similar approach has been \emph{heuristically} applied independently by several authors for estimating the mutual information \cite{poolevariational} and total correlation \cite{chen2018isolating}.
The results presented in this chapter provide strong theoretical grounding for these existing methods by showing sufficient conditions for their consistency.

Since the setting we consider arises naturally in the study of autoencoder-based generative models, the following Section \ref{sec:generative-modelling-tour} provides an overview of the key concepts in the generative modelling literature in the context of this chapter.
Following this, Section \ref{sec:theory} introduces the RAM and RAM-MC estimators and presents the main theoretical results, including rates of convergence for the bias (Theorems~\ref{thm:fast-KL-rate} and \ref{thm:convergence-rate-general}) and tail bounds (Theorems \ref{thm:concentration} and \ref{thm:mc-variance}).
In Section \ref{sec:experiments} we validate our results in both synthetic and real-data experiments. 
In Section \ref{sec:applications} we discuss further applications of our results.
We conclude in Section \ref{sec:conclusion}.
Additional details and proofs can be found in Appendix \ref{chapter:appendix-latex-space-learning-theory}.



\section{Random mixture estimator and convergence results}\label{sec:theory}

In this section we introduce our $f$-divergence estimator, and present theoretical guarantees for it.
We assume the existence of probability distributions
${P_Z}$ and ${Q_Z}$ defined over $\Z$ with known density $p(z)$ and intractable density ${q(z) = \int q(z|x) q(x) dx}$ respectively,  where ${Q_{Z|X}}$ is known. $Q_X$ defined over $\X$ is unknown, however we have an i.i.d.\:sample ${\XN=\{X_1, \ldots, X_N\}}$ from it.
Our ultimate goal is to estimate the intractable $f$-divergence
%
\begin{align*}
    D_f(Q_Z , P_Z) = \int f \left( \frac{q(z)}{p(z)} \right) p(z) dz.
\end{align*}
%
In our setting $Q_Z$ is intractable and so is ${D_f(Q_Z , P_Z)}$.
Substituting $Q_Z$ with a sample-based finite mixture ${\hat{Q}_Z^N := \frac{1}{N} \sum_{i=1}^N Q_{Z|X_i}}$ leads to our proposed 
{\bf Random Mixture estimator (RAM)}:
%
\begin{align}\textstyle
    D_f\bigl(\hat{Q}_Z^N , P_Z\bigr) := D_f\Big(\frac{1}{N} \sum_{i=1}^N Q_{Z|X_i} , P_Z\Big).
\end{align}
%
Although $\smash{\hat{Q}_Z^N}$ is a function of $\smash{\XN}$ we omit this dependence in notation for brevity. 
In this section we identify sufficient conditions under which $\smash{D_f(\hat{Q}_Z^N , P_Z)}$ is a ``good'' estimator of $\smash{D_f(Q_{Z} , P_Z)}$.
More formally, we establish conditions under which the estimator is asymptotically unbiased, concentrates to its expected value and can be practically estimated using Monte-Carlo sampling.

\subsection{Convergence rates for the bias of RAM}
The following proposition shows that $D_f(\hat{Q}_Z^N , P_Z)$ upper bounds $D_f(Q_{Z} , P_Z)$ in expectation for any finite $N$, and that the upper bound becomes tighter with increasing $N$:
%
\begin{proposition}\label{prop:upper-bound}
Let $M \leq N$ be integers. Then
\begin{align}
\label{eq:our-estimate}
    D_f(Q_Z , P_Z) \ \leq 
    \mathbb{E}_{\mathbf{X}^N} \bigl[D_f(\hat{Q}_Z^N , P_Z)\bigr] \  \leq \ \mathbb{E}_{\mathbf{X}^M} \bigl[D_f(\hat{Q}_Z^M , P_Z)\bigr].
\end{align}
\end{proposition}
\begin{proof}[Proof sketch (full proof in Appendix \ref{proof:prop1})]
The first inequality follows from Jensen's inequality, using the facts that $f$ is convex and ${Q_Z = \E_{\XN} [\hat{Q}_Z^N}]$.
The second holds since a sample ${\XM}$ can be drawn by sub-sampling (without replacement) $M$ entries of ${\XN}$, and by applying Jensen's inequality again.
\end{proof}
As a function of $N$, the expectation is a decreasing sequence that is bounded below.
By the monotone convergence theorem, the sequence converges.
Theorems \ref{thm:fast-KL-rate} and \ref{thm:convergence-rate-general} in this section give sufficient conditions under which the expectation of RAM converges to $D_f(Q_{Z} , P_Z)$ as $N\to\infty$ for a variety of $f$ and provide rates at which this happens, summarized in Table \ref{table:convergence}.
The two theorems are proved using different techniques and assumptions. 
These assumptions, along with those of existing methods (see Table~\ref{table:convergence-other}) are discussed at the end of this section.

\renewcommand{\arraystretch}{1}
\begin{table}
 \caption{Rate of bias $\E_{\XN} D_f\big(\hat{Q}^N_{Z} , P_Z\big) - D_f\left(Q_{Z} , P_Z\right)$.}
 \label{table:convergence}
 \centering
 \begin{tabular}{c c c c c c c c c } 
 \toprule
 \multirow{2}{*}{$f$-divergence} & \multirow{2}{*}{KL} & \multirow{2}{*}{TV} & \multirow{2}{*}{$\chi^2$} & \multirow{2}{*}{$\text{H}^2$} & \multirow{2}{*}{JS} & \multicolumn{2}{c}{\thead{$D_{f_\beta}$}}  & \thead{$D_{f_\alpha}$} \\ [-0.8ex]
 & & & & & & $\scriptstyle{\frac{1}{2}<\beta<1}$ & $\scriptstyle{1<\beta<\infty}$ &
$\scriptstyle{-1<\alpha<1}$ \\
 \midrule
 \thead{Theorem 1} & $\scriptstyle{N^{-1}}$ & $\scriptstyle{N^{-\frac{1}{2}}}$ & - & $\scriptstyle{N^{-\frac{1}{2}}}$ & $\scriptstyle{N^{-\frac{1}{4}}}$ & $\scriptstyle{N^{-\frac{1}{4}}}$ & $\scriptstyle{N^{-\frac{1}{4}}}$ & - \\ 
 \thead{Theorem 2} & $\scriptstyle{N^{-\frac{1}{3}}\log N}$ & $\scriptstyle{N^{-\frac{1}{2}}}$ & $\scriptstyle{N^{-1}}$ & $\scriptstyle{N^{-\frac{1}{5}}}$ & $\scriptstyle{N^{-\frac{1}{3}}\log N}$ & $\scriptstyle{N^{-\frac{1}{3}}}$ & $\scriptstyle{N^{-\frac{1}{2}}}$ & $\scriptstyle{N^{-\frac{\alpha+1}{\alpha+5}}}$ \\
 \bottomrule
\end{tabular}
\end{table}

\begin{theorem}[Rates of the bias]\label{thm:fast-KL-rate}
If
$\E_{X\sim Q_X}\bigl[\chi^2\bigl(Q_{Z|X}, Q_Z\bigr)\bigr]$ and
$\KL\left( Q_{Z} , P_Z\right)$ are finite then the bias ${\E_{\XN}\bigl[D_f( \hat{Q}_Z^N , P_Z)\bigr] - D_f\left( Q_{Z} , P_Z\right)}$ decays with rate as given in the first row of Table~\ref{table:convergence}.
\end{theorem}
\begin{proof}[Proof sketch (full proof in Appendix \ref{appendix:subsec:thm1})]
There are two key steps to the proof. 
The first is to bound the bias by ${\E_{\XN}\big[D_f(\hat{Q}_Z^N, Q_Z)\big]}$. 
For the KL this is an equality. 
For ${D_{f_\beta}}$ this holds because for $\beta {\geq} 1/2$ it is a \emph{Hilbertian metric} and its square root satisfies the triangle inequality \citep{hein05hilbertian}.
The second step is to bound ${\E_{\XN}\bigl[D_f(\hat{Q}_Z^N, Q_Z)\bigr]}$ in terms of ${\E_{\XN}\bigl[\chi^2(\hat{Q}_Z^N, Q_Z)\bigr]}$, which is the variance of the average of $N$ i.i.d.\:random variables and therefore decomposes as ${\E_{X\sim Q_X}\bigl[\chi^2(Q_{Z|X}, Q_Z)\bigr] / N}$.
\end{proof}
%
\begin{theorem}[Rates of the bias]\label{thm:convergence-rate-general}
If $\E_{X\sim Q_X, Z\sim P_Z}\bigl[ q^4(Z|X) / p^4(Z) \bigr]$ is finite then
the bias $\E_{\XN}\bigl[D_f( \hat{Q}_Z^N , P_Z)\bigr] - D_f\left( Q_{Z} , P_Z\right)$ decays with rate as given in the second row of Table \ref{table:convergence}.
\end{theorem}
\begin{proof}[Proof sketch (full proof in Appendix \ref{proof:thm2})]
Denoting by $\hat{q}_N(z)$ the density of $\hat{Q}_Z^N$,
the proof is based on the inequality
$f\bigl(\hat{q}_N(z) / p(z)\bigr) - f\bigl(q(z) / p(z)\bigr)\leq \frac{\hat{q}_N(z) - q(z)}{p(z)} f'\bigl(\hat{q}_N(z) / p(z)\bigr)$ due to convexity of $f$, applied to the bias.
The integral of this inequality is bounded by controlling $f'$, requiring subtle treatment when $f'$ diverges when the density ratio $\hat{q}_N(z)/p(z)$ approaches zero.
\end{proof}

\subsection{Tail bounds for RAM and practical estimation with RAM-MC}

Theorems \ref{thm:fast-KL-rate} and \ref{thm:convergence-rate-general} describe the convergence of the  \emph{expectation} of RAM over $\XN$, which in practice may be intractable.
Fortunately, the following shows that RAM rapidly concentrates to its expectation.


\begin{table}
 \caption{Rate $\psi(N)$ of high probability bounds for $D_f\big(\hat{Q}^N_{Z} , P_Z\big)$ (Theorem 3).}
 \label{table:concentration}
 \centering
 \begin{tabular}{c c c c c c c c c } 
 \toprule
 \multirow{2}{*}{$f$-divergence} & \multirow{2}{*}{KL} & \multirow{2}{*}{TV} & \multirow{2}{*}{$\chi^2$} & \multirow{2}{*}{$\text{H}^2$} & \multirow{2}{*}{JS} & \multicolumn{2}{c}{\thead{$D_{f_\beta}$}}  & \thead{$D_{f_\alpha}$} \\ [-0.8ex]
 & & & & & & $\scriptstyle{\frac{1}{2}<\beta<1}$ & $\scriptstyle{1<\beta<\infty}$ &
$\scriptstyle{\frac{1}{3}<\alpha<1}$ \\
 \midrule
 \thead{$\psi(N)$} &  $\scriptstyle{N^{-\frac{1}{6}}\log N}$ & $\scriptstyle{N^{-\frac{1}{2}}}$ & 
 $\scriptstyle{N^{-\frac{1}{2}}}$ &
 - & 
 $\scriptstyle{N^{-\frac{1}{6}}\log N}$ &
 $\scriptstyle{N^{-\frac{1}{6}}}$ &
 $\scriptstyle{N^{-\frac{1}{2}}}$ &
 $\scriptstyle{N^{\frac{1-3\alpha}{\alpha+5}}}$
 \\ 
 \bottomrule
\end{tabular}
\end{table}

\begin{theorem}[Tail bounds for RAM]\label{thm:concentration}
Suppose that ${\chi^2\left(Q_{Z|x} , P_Z\right) \leq C < \infty}$ for all $x$ and for some constant $C$.
Then, the RAM estimator ${D_f( \hat{Q}_Z^N , P_Z)}$ concentrates to its mean in the following sense. 
For $N>8$ and for any $\delta >0$, with probability at least $1-\delta$ it holds that
\begin{align*}
    \left| D_f( \hat{Q}_Z^N , P_Z) - \mathbb{E}_{\XN} \bigl[D_f(\hat{Q}_Z^N , P_Z)\bigr] \right| \leq {K \cdot \psi(N)} \  \sqrt{\log (2/\delta)},
\end{align*}
where $K$ is a constant and $\psi(N)$ is given in Table~\ref{table:concentration}.
\end{theorem}
\begin{proof}[Proof sketch (full proof in Appendix \ref{proof:thm3})]
These results follow by applying McDiarmid's inequality.
To apply it we need to show that RAM viewed as a function of $\XN$ has bounded differences.
We show that when replacing $\smash{X_i\in\XN}$ with $\smash{X_i'}$ the value of $\smash{D_f( \hat{Q}_Z^N , P_Z)}$ changes by at most $\smash{O(N^{-1/2}\psi(N))}$.
Proof of this proceeds similarly to the one of Theorem \ref{thm:convergence-rate-general}.
\end{proof}

In practice it may not be possible to evaluate $\smash{D_f( \hat{Q}_Z^N , P_Z)}$ analytically. 
We propose to use Monte-Carlo (MC) estimation since both densities $\hat{q}_N(z)$ and $p(z)$ are assumed to be known.
We consider importance sampling with proposal distribution ${\pi(z|\XN)}$, highlighting the fact that $\pi$ can depend on the sample $\XN$.
If $\pi(z|\XN) = p(z)$ this reduces to normal MC sampling. 
We arrive at the {\bf RAM-MC estimator} based on $M$ i.i.d.\:samples $\ZM:=\{Z_1,\dots,Z_M\}$ from $\pi(z|\XN)$:
\begin{align}
\label{eq:our-mc-estimate}
    %\phi_\pi\left(\ZM, \XN\right) := 
    \hat{D}^M_f( \hat{Q}_Z^N , P_Z) :=
    \frac{1}{M}\sum_{m=1}^M f\left( \frac{\hat{q}_N(Z_m)}{p(Z_m)} \right) \frac{p(Z_m)}{\pi\left(Z_m|\XN\right)}.
\end{align}

\begin{theorem}[RAM-MC is unbiased and consistent]\label{thm:mc-variance}
$\E\bigl[\hat{D}^M_f( \hat{Q}_Z^N , P_Z)\bigr]
=\E\bigl[D_f( \hat{Q}^N_{Z} , P_Z )\bigr]$ for any proposal distribution $\pi$.
If $\pi(z|\XN) = p(z)$ or $\pi(z | \XN) = \hat{q}_N(z)$ then under mild assumptions$^\star$ on the moments of $q(Z|X)/p(Z)$
and denoting by ${\psi(N)}$ the rate given in Table~\ref{table:concentration}, we have
\begin{align*}
    \text{Var}_{\XN, \ZM} \bigl[\hat{D}^M_f( \hat{Q}_Z^N , P_Z)\bigr] = 
    O\left(M^{-1}\right) + O\left( \psi(N)^2 \right).
\end{align*}
\end{theorem}
\begin{proof}[Proof sketch ($^\star$full statement and proof in Appendix \ref{appendix:full-statment-proof-mc})]
By the law of total variance, 
\begin{align*}
    \text{Var}_{\XN, \ZM} \bigl[\hat{D}^M_f\bigr] = 
    \mathbb{E}_{\XN} \bigl[\text{Var}\bigl[\hat{D}^M_f\, | \XN\bigr]\bigr] + \text{Var}_{\XN} \bigl[D_f( \hat{Q}_Z^N , P_Z )\bigr].
\end{align*}
The first of these terms is ${O( M^{-1})}$ by standard results on MC integration, subject to the assumptions on the moments.
Using the fact that ${\text{Var}[Y] = \int_0^\infty\mathbb{P} ( |Y - \mathbb{E} Y| > \sqrt{t}) dt}$ for any random variable $Y$
we bound the second term by integrating the exponential tail bound of Theorem~\ref{thm:concentration}.
\end{proof}

Through use of the Efron-Stein inequality---rather than integrating the tail bound provided by McDiarmid's inequality---it is possible for some choices of $f$ to weaken the assumptions under which the $O(\psi(N)^2)$ variance is achieved: from uniform boundedness of $\smash{\chi^2(Q_{Z|X},P_Z)}$ to boundedness in expectation.
In general, a variance better than ${O(M^{-1})}$ is not possible using importance sampling. However, the constant and hence practical performance may vary significantly depending on the choice of $\pi$.
We note in passing that through Chebyshev's inequality, it is possible to derive confidence bounds for RAM-MC of the form similar to Theorem~\ref{thm:concentration}, but with an additional dependence on $M$ and worse dependence on $\delta$. 
For brevity we omit this.


\renewcommand{\arraystretch}{1}
\begin{table}
 \caption{Rate of bias for other estimators of $D_f(P,Q)$.}
 \label{table:convergence-other}
 \centering
 \begin{tabular}{c c c c c c c c c } 
 \toprule
 \multirow{2}{*}{$f$-divergence} & \multirow{2}{*}{KL} & \multirow{2}{*}{TV} & \multirow{2}{*}{$\chi^2$} & \multirow{2}{*}{$\text{H}^2$} & \multirow{2}{*}{JS} & \multicolumn{2}{c}{\thead{$D_{f_\beta}$}}  & \thead{$D_{f_\alpha}$} \\ [-0.8ex]
 & & & & & & $\scriptstyle{\frac{1}{2}<\beta<1}$ & $\scriptstyle{1<\beta<\infty}$ &
$\scriptstyle{-1<\alpha<1}$ \\
 \midrule
 \thead{Krishnamurthy et al. [22]} & - & - & - & - & - & - & - & $\scriptstyle{N^{-\frac{1}{2}} + N^{\frac{-3s}{2s + d}}}$ \\ 
 \thead{Nguyen et al. [28]} & $\scriptstyle{N^{-\frac{1}{2}}}$ & - & - & - & - & - & - & - \\ 
 \thead{Moon and Hero [26]} & $\scriptstyle{N^{-\frac{1}{2}}}$ & - & $\scriptstyle{N^{-\frac{1}{2}}}$ & $\scriptstyle{N^{-\frac{1}{2}}}$ & $\scriptstyle{N^{-\frac{1}{2}}}$ & $\scriptstyle{N^{-\frac{1}{2}}}$ & $\scriptstyle{N^{-\frac{1}{2}}}$ & $\scriptstyle{N^{-\frac{1}{2}}}$ \\ 
 \bottomrule
\end{tabular}
\end{table}

\subsection{Discussion: assumptions and summary}\label{subsection:discussion-assumptions}
All the rates in this section are independent of the dimension of the space $\mathcal{Z}$ over which the distributions are defined.
However the constants may exhibit some dependence on the dimension.
Accordingly, for fixed $N$, the bias and variance may generally grow with the dimension.

Although the data distribution $Q_X$ will generally be unknown, in some practical scenarios such as deep autoencoder models, $P_Z$ may be chosen by design and $Q_{Z|X}$ learned subject to architectural constraints.
In such cases, the assumptions of Theorems \ref{thm:convergence-rate-general} and \ref{thm:concentration} can be satisfied by making suitable restrictions (we conjecture also for Theorem~\ref{thm:fast-KL-rate}).
For example, suppose that ${P_Z}$ is  ${\mathcal{N}\left(0, I_d\right)}$ and ${Q_{Z|X}}$ is  ${\mathcal{N}\left( \mu(X), \Sigma(X)\right)}$ with $\Sigma$ diagonal. 
Then the assumptions hold if there exist constants $K, \epsilon > 0$ such that ${\| \mu(X)\| < K}$ and ${\Sigma_{ii}(X) \in [\epsilon, 1]}$ for all $i$ (see Appendix \ref{appendix:discussion-constraints}).
In practice, numerical stability often requires the diagonal entries of $\Sigma$ to be lower bounded by a small number (e.g. $10^{-6}$).
If $\mathcal{X}$ is compact (as for images) then such a $K$ is guaranteed to exist; if not, choosing $K$ very large yields an insignificant constraint.

Table~\ref{table:convergence-other} summarizes the rates of bias for some existing methods.
In contrast to our proposal, the assumptions of these estimators may in practice be difficult to verify.
For the estimator of \cite{krishnamurthy14icml}, both densities $p$ and $q$ must belong to the H\"older class of smoothness $s$, be supported on $[0,1]^d$ and satisfy $0<\eta_1 < p, q < \eta_2<\infty$ on the support for known constants $\eta_1, \eta_2$.
For that of \cite{nguyen10ratio}, the density ratio $p/q$ must satisfy $0<\eta_1 < p/q < \eta_2<\infty$ and belong to a function class $G$ whose \emph{bracketing entropy} (a measure of the complexity of a function class) is properly bounded. The condition on the bracketing entropy is quite strong and ensures that the density ratio is well behaved.
For the estimator of \cite{moon14ensemble}, both $p$ and $q$ must have the same bounded support and satisfy $0<\eta_1 < p, q < \eta_2<\infty$ on the support. $p$ and $q$ must have \emph{continuous bounded} derivatives of order $d$ (which is stronger than assumptions of [22]), and $f$ must have derivatives of order at least $d$.

In summary, the RAM estimator $D_f(\hat{Q}_Z^N , P_Z)$ for $D_f(Q_Z , P_Z)$ is \textbf{consistent} since it concentrates to its expectation $\E_{\XN}\bigl[D_f(\hat{Q}_Z^N , P_Z)\bigr]$, which in turn converges to $D_f(Q_Z , P_Z)$.
It is also \textbf{practical} because it can be efficiently estimated with Monte-Carlo sampling via RAM-MC.

\section{Empirical evaluation}\label{sec:experiments}

In the previous section we showed that our proposed estimator has a number of desirable theoretical properties.
Next we demonstrate its practical performance.
First, we present a synthetic experiment investigating the behaviour of RAM-MC in controlled settings where all distributions and divergences are known.
Second, we investigate the use of RAM-MC in a more realistic setting to estimate a divergence between the aggregate posterior $Q_Z$ and prior $P_Z$ in pretrained autoencoder models. 
For experimental details not included in the main text,
see Appendix \ref{appendix:empirical-evaluation-details}.
%\footnote{A python notebook to reproduce all experiments is available at \url{https://github.com/google-research/google-research/tree/master/f_divergence_estimation_ram_mc}.}


\subsection{Synthetic experiments}\label{section:synth-exps}
%\textbf{The data model.}
Our goal in this subsection is to test the behaviour of the RAM-MC estimator for various $d=\dim(\mathcal{Z})$ and $f$-divergences.
We choose a setting in which $Q^{\lambda}_Z$ parametrized by a scalar $\lambda$ and $P_Z$ are both $d$-variate normal distributions for $d\in\{1, 4, 16\}$.
We use RAM-MC to estimate $D_f(Q^\lambda_Z, P_Z)$, which can be computed analytically for the KL, $\chi^2$, and squared Hellinger divergences in this setting (see Appendix \ref{appendix:toy-exps}).
Namely, we take ${P_Z}$ and ${Q_X}$ to be standard normal distributions over $\Z=\R^d$ and $\X=\R^{20}$ respectively,
and $\smash{Z\sim Q^\lambda_{Z|X}}$ be a linear transform of $X$ plus a fixed isotropic Gaussian noise, with the linear function parameterized by $\lambda$.
By varying $\lambda$ we can interpolate between different values for $D_f(Q_Z^\lambda , P_Z)$.

%\textbf{The estimators.}
In Figure \ref{fig:synthetic-exps} we show the behaviour of RAM-MC with $N\,{\in}\,\{1, 500\}$ and $M{=}128$ compared to the ground truth as $\lambda$ is varied. 
The columns of Figure~\ref{fig:synthetic-exps} correspond to different dimensions $d\,{\in}\,\{1, 4, 16\}$, and rows to the $\KL$, $\chi^2$ and $\mathrm{H}^2$ divergences, respectively. 
We also include two baseline methods.
First, a plug-in method based on kernel density estimation \cite{moon14ensemble}.
Second, and only for the KL case, the M1 method of~\cite{nguyen10ratio} based on density ratio estimation.

%\textbf{The experiment.}
To produce each plot, the following was performed 10 times, with the mean result giving the bold lines and standard deviation giving the error bars.
First, $N$ points $\XN$ were drawn from $Q_X$. 
Then $M{=}128$ points $\ZM$ were drawn from $\hat{Q}_Z^N$ and RAM-MC \eqref{eq:our-mc-estimate} was evaluated. 
For the plug-in estimator, the densities $\hat{q}(z)$ and $\hat{p}(z)$ were estimated by kernel density estimation with 500 samples from $Q_Z$ and $P_Z$ respectively using the default settings of the Python library {\texttt{scipy.stats.gaussian\_kde}}.
The divergence was then estimated via MC-sampling using $128$ samples from $Q_Z$ and the surrogate densities.
The M1~estimator involves solving a convex linear program in $N$ variables to maximize a lower bound on the true divergence, see \cite{nguyen10ratio} for more details.
Although the M1~estimator can in principle be used for arbitrary $f$-divergences, its implementation requires hand-crafted derivations that are supplied only for the $\KL$ in \cite{nguyen10ratio}, which are the ones we use.

%\textbf{Discussion.}
The results of this experiment empirically support Proposition \ref{prop:upper-bound} and Theorems \ref{thm:fast-KL-rate}, \ref{thm:convergence-rate-general}, and~\ref{thm:mc-variance}:
(i) in expectation, RAM-MC upper bounds the true divergence; (ii) by increasing $N$ from 1 to 500 we clearly decrease both the bias and the variance of RAM-MC.
When the dimension $d$ increases, the bias for fixed $N$ also increases.
This is consistent with the theory in that, although the rates are independent of $d$, the constants are not.
We note that by side-stepping the issue of density estimation, RAM-MC performs favourably compared to the plug-in and M1 estimators, more so in higher dimensions ($d=16$).
In particular, the shape of the RAM-MC curve follows that of the truth for each divergence, while that of the plug-in estimator does not for larger dimensions.
In some cases the plug-in estimator can even take negative values because of the large variance.


\begin{figure}
\begin{center}
\begin{tikzpicture}
\node[anchor=south west,inner sep=0] at (0,0) {\includegraphics[width=0.97\textwidth, height=0.615\textwidth]{pics/NeurIPS_toy_exps_plot.pdf}};
\node[rotate=0] at (0, 1.7) {$\mathrm{H}^2$};
\node[rotate=0] at (0, 4.2) {$\chi^2$};
\node[rotate=0] at (0, 6.7) {$\mathrm{KL}$};
\end{tikzpicture}
\end{center}
\caption{\label{fig:synthetic-exps}
(Section~\ref{section:synth-exps})
Estimating $D_f\bigl(\mathcal{N}(\mu_\lambda, \Sigma_\lambda),\, \mathcal{N}(0, I_d)\bigr)$ for various $f$, $d$, and parameters $\mu_\lambda$ and $\Sigma_\lambda$ indexed by $\lambda\in \R$.
Horizontal axis correspond to $\lambda\in[-2, 2]$,
columns to $d\in\{1, 4, 16\}$ and
rows to KL, $\chi^2$, and $\mathrm{H}^2$ divergences respectively.
{\bf \textcolor{blue}{Blue}} are true divergences, 
{\bf black} and {\bf \textcolor{red}{red}} are RAM-MC estimators~\eqref{eq:our-mc-estimate} for $N\in\{1, 500\}$ respectively,
{\bf \textcolor{darkgreen}{green}} are M1 estimator of~\citep{nguyen10ratio} and {\bf \textcolor{orange}{orange}} are plug-in estimates based on Gaussian kernel density estimation \citep{moon14ensemble}.
$N=500$ and $M=128$ in all the plots if not specified otherwise.
Error bars depict one standard deviation over 10 experiments.
}
\end{figure}

\subsection{Real-data experiments}
\label{sec:exp_wae}
%\textbf{The data model.}
To investigate the behaviour of RAM-MC in a more realistic setting, we consider VAEs and WAEs.
Recall that both models have a prior ${P_Z}$ over the latent space and involve learning an \emph{encoder} $\smash{Q^\phi_{Z|X}}$ with parameter $\phi$ mapping from the data to latent space.
%A prior distribution ${P_Z}$ is specified, and the 
%optimization objectives of both models are of the form ``reconstruction + distribution matching penalty''.
%The penalty of the VAE was shown by \cite{hoffman2016elbo} to be equivalent to $\smash{\KL(Q^\theta_Z \| P_Z) + I(X,Z)}$ where $I(X,Z)$ is the mutual information of a sample and its encoding.
%The WAE penalty is ${D(Q^\theta_Z \| P_Z)}$ for any divergence $D$ that can practically be estimated.
%Following \cite{tolstikhin2017wasserstein}, we trained models using the Maximum Mean Discrepency (MMD), a kernel-based distance on distributions, and a divergence estimated using a GAN-style classifier leading to WAE-MMD and WAE-GAN respectively \cite{gretton2012kernel, goodfellow2014generative}.
% Both of them can be estimated from samples.
%For more information about VAE and WAE, see Appendix \ref{appendix:intro-vae-wae}.
%\textbf{The experiment.}
We consider models pre-trained on the \emph{CelebA} dataset \cite{liu2015faceattributes}, 
and use them to evaluate the RAM-MC estimator as follows.
We take the test dataset as the ground-truth $Q_X$, and embed it into the latent space via the trained encoder.
All models considered have Gaussian encoders.
As a result, we obtain a ${\sim}{20}\text{k}$-component Gaussian mixture for $Q_Z$, the \emph{empirical aggregate posterior}. 
Since $Q_Z$ is a finite---not continuous--- mixture, the true $D_f(Q_Z,P_Z)$ can be estimated using a large number of MC samples (we used $10^4$).
Note that this is very costly and involves evaluating $2\cdot 10^4$ Gaussian densities for each of the $10^4$ MC points.
We repeated this evaluation 10 times and report means and standard deviations.
RAM-MC is evaluated using $N \in \{2^0, 2^1,\ldots, 2^{14}\}$ and $M \in \{10, 10^3\}$.
For each combination $(N,M)$, RAM-MC was computed 50 times with the means plotted as bold lines and standard deviations as error bars.
In Figure~\ref{fig:real-exps} we show the result of performing this for the KL divergence on six different models.
For each dimension $d\in\{32, 64, 128\}$, we chose two models from the classes VAE, WAE-MMD and WAE-GAN. 
See Appendix~\ref{appendix:real-data-experiments-additional} for further details and similar plots for the $H^2$-divergence.

%\textbf{Discussion.}
The results are encouraging. 
In all cases RAM-MC achieves a reasonable accuracy with $N$ relatively small, even for the bottom right model where the true KL divergence ($\approx 1910$) is very big.
We see evidence supporting Theorem~\ref{thm:mc-variance}, which says that the variance of RAM-MC is mostly determined by the smaller of $\psi(N)$ and $M$:
when $N$ is small, the variance of RAM-MC does not change significantly with $M$, 
however when $N$ is large, increasing $M$ significantly reduces the variance. 
Also we found there to be two general modes of behaviour of RAM-MC across the six trained models we considered. 
In the bottom row of Figure~\ref{fig:real-exps} we see that the decrease in bias with $N$ is very obvious, supporting Proposition~\ref{prop:upper-bound} and Theorems \ref{thm:fast-KL-rate} and \ref{thm:convergence-rate-general}.
In contrast, in the top row it is less obvious, because the comparatively larger variance for $M{=}10$ dominates reductions in the bias.
Even in this case, both the bias and variance of RAM-MC with $M{=}1000$ become negligible for large $N$.
Importantly, the behaviour of RAM-MC does not degrade in higher dimensions.


The baseline estimators (plug-in \cite{moon14ensemble} and M1~\cite{nguyen10ratio}) perform so poorly that we decided not to include them in the plots (doing so would distort the $y$-axis scale).
In contrast, even with a relatively modest $N{=}2^8$ and $M{=}1000$ samples, RAM-MC behaves reasonably well in all cases.

\begin{figure}
\begin{center}
\includegraphics[width=1.\textwidth, height=0.615\textwidth]{pics/NeurIPS_wae_exps_plot.pdf}
\end{center}
\caption{\label{fig:real-exps}
(Section~\ref{sec:exp_wae}) Estimates of $KL(Q_Z^\theta , P_Z)$ for pretrained autoencoder models with RAM-MC as a function of $N$ for $M{=}10$ ({\bf \textcolor{green!65!blue}{green}}) and $M{=}1000$ ({\bf \textcolor{red}{red}}) compared to an accurate MC estimate of the ground truth ({\bf\textcolor{blue}{blue}}).
Lines and error bars represent means and standard deviations over 50 trials.
}
\end{figure}



\section{Applications: total correlation, entropy, and mutual information estimates}\label{sec:applications}
This section details some direct consequences of the proposed estimator and its guarantees.
Our theory may also apply to a number of machine learning domains where estimating entropy, total correlation or mutual information is either the final goal or part of a broader optimization loop.
\paragraph{Total correlation and entropy estimation.}
The differential entropy, defined as $H(Q_Z)= -\int_{\mathcal{Z}} q(z) \log q(z)  dz$, is often a quantity of interest in machine learning.
While this is intractable in general, straightforward computation shows that for \emph{any} $P_Z$
{\addtolength{\abovedisplayskip}{-0.5mm}
\addtolength{\belowdisplayskip}{-0.5mm}
\begin{align*}
    H(Q_Z) - \mathbb{E}_{\XN} H(\hat{Q}_Z^N) = \mathbb{E}_{\XN} D_{\text{KL}}\left(\hat{Q}_Z^N , P_Z\right) -  D_{\text{KL}}\left(Q_Z , P_Z\right).
\end{align*}}%
Therefore, our results provide sufficient conditions under which $\smash{H(\hat{Q}_Z^N)}$ converges to $\smash{H(Q_{Z})}$ and concentrates to its mean.
This result has consequences for some existing VAE literature.
The Total Correlation (TC) of a distribution $Q_Z$ is defined as 
%
\begin{align*}
TC(Q_Z) := D_{\text{KL}}\left(Q_Z, \prod_{i=1}^{d_Z} Q_{Z_i}\right) = \sum_{i=1}^{d_Z}H(Q_{Z_i}) - H(Q_Z)
\end{align*}
%
where $Q_{Z_i}$ is the $i$th marginal of $Q_Z$.
This is considered by \cite{chen2018isolating}, who add it to the VAE loss function to encourage $Q_Z$ to be factorized, resulting in the $\beta$-TC-VAE algorithm.
By the second equality above, estimation of TC can be reduced to estimation of $H(Q_Z)$ with only slight modifications needed to treat $H(Q_{Z_i})$.

Two methods are proposed by \cite{chen2018isolating} for estimating $\smash{H(Q_Z)}$, both of which assume a finite dataset of size $D$.
One of these, named \emph{Minibatch Weighted Sample} (MWS), coincides with $\smash{H(\hat{Q}_Z^N) + \log D}$ estimated with a particular form of MC sampling.
Our results therefore imply \emph{inconsistency} of the MWS method due to the constant $\log D$ offset. 
In the context of \cite{chen2018isolating} this is not actually problematic since a constant offset does not affect gradient-based optimization techniques.
Interestingly, although their derivations suppose a data distribution of finite support, our results show that minor modifications result in an estimator suitable for both finite and infinite support data distributions.

\paragraph{Mutual information estimation.}
The mutual information (MI) between variables with joint distribution $\smash{Q_{Z,X}}$ is defined as $\smash{I(Z, X) := D_{\text{KL}}\left(Q_{Z,X}, Q_Z Q_X \right) = \E_{X} D_{\text{KL}}\left(Q_{Z|X}, Q_Z \right)}$.
Several recent papers have estimated or optimized this quantity in the context of autoencoder architectures, coinciding with our setting \cite{dieng2018avoiding, hoffman2016elbo, alemi2017fixing, oord2018representation}. In particular, \cite{poolevariational} propose the following estimator based on replacing $Q_Z$ with $\smash{\hat{Q}_{Z}^N}$, proving it to be a lower bound on the true MI:
%
\begin{align*}\textstyle
    I_{TCPC}^N(Z,X) = \mathbb{E}_{\XN}\Big[\frac{1}{N} \sum_{i=1}^N D_{\text{KL}}\left( Q_{Z|X_i}, \hat{Q}_{Z}^N \right)\Big] \leq I(Z,X).
\end{align*}
%
The gap in this inequality can be written as
%
\begin{align*}
I(Z,X) - I_{TCPC}^N(Z,X) = \E_{\XN} D_{\text{KL}}\left( \hat{Q}_{Z}^N, P_{Z} \right) - D_{\text{KL}}\left( Q_{Z}, P_Z \right)
\end{align*}
%
where $P_Z$ is \emph{any} distribution. 
Therefore, our results also provide sufficient conditions under which $\smash{I^N_{TCPC}}$ converges and concentrates to the true mutual information.


%\todo{Rewrite section heading to make this next bit fit in, moved from the intro}
\paragraph{Related, but fundamentally different work}
\cite{burda2015importance} propose to reduce the gap introduced by Jensen's inequality in the derivation of the classical evidence lower bound (ELBO) by using multiple Monte-Carlo samples from the approximate posterior $\smash{Q_{Z|X}}$.
This is similar in flavour to the approach considered in this chapter, but is fundamentally different since we use multiple samples from the \emph{data distribution} to reduce a different Jensen gap.
To avoid confusion, note that replacing the `regularizer' term $\mathbb{E}_X[D_{\text{KL}}(Q_{Z|X}, P_Z)]$ of the classical ELBO with expectation of our estimator $\E_{\XN}[D_{\text{KL}}(\hat{Q}_Z^N, P_Z)]$ results in an upper bound of the classical ELBO (by Proposition~\ref{prop:upper-bound}) but is itself not in general an evidence lower bound:
%
\begin{align*}
    \mathbb{E}_X \Big[ \mathbb{E}_{Q_{Z|X}} \log p(X|Z) - D_{\text{KL}}(Q_{Z|X}, P_Z ) \Big] \leq \mathbb{E}_X \Big[ \mathbb{E}_{Q_{Z|X}} \log p(X|Z) \Big] - \mathbb{E}_{\XN} \Big[ D_{\text{KL}}(\hat{Q}_Z^N, P_Z ) \Big].
\end{align*}


\section{Conclusion}\label{sec:conclusion}
This chapter introduced a practical estimator for the $\smash{f}$-divergence $D_f(Q_Z,P_Z)$ where $Q_Z = \int Q_{Z|X}dQ_X$, samples from $Q_X$ are available, and $P_Z$ and $Q_{Z|X}$ have known density.
The RAM estimator is based on approximating the true $Q_Z$ with data samples as a random mixture via $\smash{\hat{Q}^N_{Z}=\frac{1}{N}\sum_{n} Q_{Z|X_n}}$.
We denote by RAM-MC the estimator version where $\smash{D_f(\hat{Q}^N_Z,P_Z)}$ is estimated with MC sampling.
We proved rates of convergence and concentration for both RAM and RAM-MC, in terms of sample size $N$ and MC samples $M$ under a variety of choices of $\smash{f}$.
Synthetic and real-data experiments strongly support the validity of our proposal in practice, and our theoretical results provide guarantees for methods previously proposed heuristically in existing literature.

Future work could investigate the use of our proposals for optimization loops, in contrast to pure estimation.
When $\smash{Q^\phi_{Z|X}}$ depends on parameter $\phi$ and the goal is to minimize $\smash{D_f(Q_Z^\phi , P_Z)}$ with respect to $\phi$ such as is the case for Wasserstein Autoencoders, RAM-MC provides a practical surrogate loss that can be minimized using stochastic gradient methods.





