%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Third Chapter **********************************
%*******************************************************************************
\chapter{Generative Modelling}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter5/Figs/Raster/}{Chapter5/Figs/PDF/}{Chapter5/Figs/}}
\else
    \graphicspath{{Chapter5/Figs/Vector/}{Chapter5/Figs/}}
\fi

\emph{This chapter is largely based on the paper:}

\begin{quote}
\fullcite{rubenstein2019practical}
\end{quote}

\emph{and in part the following workshop paper:}

\begin{quote}
\fullcite{rubenstein2018wasserstein}
\end{quote}


\emph{Additionally, the following workshop paper on the theme of generative modelling was published during my PhD but is not included in this thesis:}

\begin{quote}
\fullcite{rubenstein2018learning}
\end{quote}

\emph{Sections ?? - ?? are a review of latent variable models used in the generative modelling community, in particular Wasserstein Autoencoders (WAEs). 
This sets the stage for Sections ?? - ?? which is based on} \cite{rubenstein2019practical}.
\emph{This is a learning theoretic analysis of one aspect of training WAEs.}


\section{Introduction}

Generative modelling has in recent years become synonymous with the artificial generation of natural images or audio \cite{something_for_images, wavenet?}.
The mathematical formulation of this is, however, more general.
Suppose that we are given a dataset of samples. 
These are presumed to have been drawn \iid~from some unknown data distribution $Q_X$.

The high level goal of generative modelling is to learn a distribution $P_X \approx Q_X$ from which new samples can be drawn. 
To make this precise, one must specify a choice of \emph{divergence} $D$ and family of distributions $P_X^\theta$ with parameter $\theta\in\Theta$, and the goal becomes:

\begin{align*}
\min_{\theta \in \Theta} D\left(P_X^\theta | Q_X \right)
\end{align*}

There are two main challenges.
First, when the data are drawn from complex high dimensional distributions, how can this be modelled with a parameterised distribution?
Second, what are appropriate choices of divergences, and how can they be estimated or minimised with respect to the parameters $\theta$?

We will tackle these two questions in the next parts. 

\subsection{Latent variable models}

\subsection{Divergences}


\section{Examples of generative models}

\subsection{GANs}
\subsection{VAEs}
\subsection{WAEs}

