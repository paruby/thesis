@article{csiszar2004information,
  title={Information theory and statistics: A tutorial},
  author={Csisz{\'a}r, Imre and Shields, Paul C and others},
  journal={Foundations and Trends{\textregistered} in Communications and Information Theory},
  volume={1},
  number={4},
  pages={417--528},
  year={2004},
  publisher={Now Publishers, Inc.}
}

@article{chen2018isolating,
  title={Isolating Sources of Disentanglement in Variational Autoencoders},
  author={Chen, Tian Qi and Li, Xuechen and Grosse, Roger and Duvenaud, David},
  journal={arXiv preprint arXiv:1802.04942},
  year={2018}
}

@article{lin2019wise,
  title={WiSE-VAE: Wide Sample Estimator VAE},
  author={Lin, Shuyu and Clark, Ronald and Birke, Robert and Trigoni, Niki and Roberts, Stephen},
  journal={arXiv preprint arXiv:1902.06160},
  year={2019}
}

@InProceedings{poolevariational,
  title={On variational lower bounds of mutual information},
  author={Poole, Ben and Ozair, Sherjil and van den Oord, A{\"a}ron and Alemi, Alexander A and Tucker, George},
  booktitle = {ICML},
  year = {2018}
}

@InProceedings{tolstikhin2017wasserstein,
  title={Wasserstein auto-encoders},
  author={Tolstikhin, Ilya and Bousquet, Olivier and Gelly, Sylvain and Schoelkopf, Bernhard},
  booktitle={ICLR},
  year={2018}
}

@article{rubenstein2018latent,
  title={On the Latent Space of Wasserstein Auto-Encoders},
  author={Rubenstein, Paul K and Schoelkopf, Bernhard and Tolstikhin, Ilya},
  journal={arXiv preprint arXiv:1802.03761},
  year={2018}
}

@inproceedings{nowozin2016f,
  title={f-{GAN}: Training generative neural samplers using variational divergence minimization},
  author={Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
  booktitle={Advances in neural information processing systems},
  pages={271--279},
  year={2016}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{nguyen10ratio,
  author    = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
  title     = {Estimating divergence functionals and the likelihood ratio by convex risk minimization},
  journal   = {{IEEE} Trans. Information Theory},
  volume    = {56},
  number    = {11},
  pages     = {5847--5861},
  year      = {2010},
}

@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}



@inproceedings{dieng2017variational,
  title={Variational Inference via $\chi$ Upper Bound Minimization},
  author={Dieng, Adji Bousso and Tran, Dustin and Ranganath, Rajesh and Paisley, John and Blei, David},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2732--2741},
  year={2017}
}

@inproceedings{li2016renyi,
  title={R{\'e}nyi divergence variational inference},
  author={Li, Yingzhen and Turner, Richard E},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1073--1081},
  year={2016}
}

@InProceedings{pmlr-v80-chen18k,
  title = 	 {Variational Inference and Model Selection with Generalized Evidence Bounds},
  author = 	 {Chen, Liqun and Tao, Chenyang and Zhang, Ruiyi and Henao, Ricardo and Duke, Lawrence Carin},
  booktitle = 	 {ICML},
  year = 	 {2018},
  pdf = 	 {http://proceedings.mlr.press/v80/chen18k/chen18k.pdf},
  url = 	 {http://proceedings.mlr.press/v80/chen18k.html},
  abstract = 	 {Recent advances on the scalability and flexibility of variational inference have made it successful at unravelling hidden patterns in complex data. In this work we propose a new variational bound formulation, yielding an estimator that extends beyond the conventional variational bound. It naturally subsumes the importance-weighted and Renyi bounds as special cases, and it is provably sharper than these counterparts. We also present an improved estimator for variational learning, and advocate a novel high signal-to-variance ratio update rule for the variational parameters. We discuss model-selection issues associated with existing evidence-lower-bound-based variational inference procedures, and show how to leverage the flexibility of our new formulation to address them. Empirical evidence is provided to validate our claims.}
}

@article{NielsenN14,
  author    = {Nielsen, Frank and Nock, Richard},
  title     = {On the Chi Square and Higher-Order Chi Distances for Approximating f-Divergences},
  journal   = {{IEEE} Signal Process. Lett.},
  volume    = {21},
  number    = {1},
  pages     = {10--13},
  year      = {2014},
}

@inproceedings{moon14ensemble,
  author    = {Moon, K. and Hero, A.},
  title     = {Ensemble estimation of multivariate f-divergence},
  booktitle = {2014 {IEEE} International Symposium on Information Theory},
  pages     = {356--360},
  year      = {2014},
}

@article{tsybakov2009,
author = {B. Tsybakov, Alexandre},
year = {2009},
title = {Introduction to Nonparametric Estimation},
doi = {10.1007/b13794}
}

@article{ganin2016domain,
  title={Domain-adversarial training of neural networks},
  author={Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={2096--2030},
  year={2016},
  publisher={JMLR. org}
}

@inproceedings{ben2007analysis,
  title={Analysis of representations for domain adaptation},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando},
  booktitle={Advances in neural information processing systems},
  pages={137--144},
  year={2007}
}

@article{burda2015importance,
  title={Importance weighted autoencoders},
  author={Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1509.00519},
  year={2015}
}

@article{dieng2018avoiding,
  title={Avoiding latent variable collapse with generative skip models},
  author={Dieng, Adji B and Kim, Yoon and Rush, Alexander M and Blei, David M},
  journal={arXiv preprint arXiv:1807.04863},
  year={2018}
}

@inproceedings{hoffman2016elbo,
  title={{ELBO} surgery: yet another way to carve up the variational evidence lower bound},
  author={Hoffman, Matthew D and Johnson, Matthew J},
  year={2016}
}


@inproceedings{alemi2017fixing,
  title={Fixing a Broken {ELBO}},
  author={Alemi, Alexander and Poole, Ben and Fischer, Ian and Dillon, Joshua and Saurous, Rif A and Murphy, Kevin},
  booktitle={ICML},
  pages={159--168},
  year={2018}
}

@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@article{osterreicher2003new,
  title={A new class of metric divergences on probability spaces and its applicability in statistics},
  author={Osterreicher, Ferdinand and Vajda, Igor},
  journal={Annals of the Institute of Statistical Mathematics},
  volume={55},
  number={3},
  pages={639--653},
  year={2003},
  publisher={Springer}
}

@article{liese2006divergences,
  title={On divergences and informations in statistics and information theory},
  author={Liese, Friedrich and Vajda, Igor},
  journal={IEEE Transactions on Information Theory},
  volume={52},
  number={10},
  pages={4394--4412},
  year={2006},
  publisher={IEEE}
}

@InProceedings{krishnamurthy14icml,
  title = 	 {Nonparametric estimation of {R}{\'e}nyi
divergence and friends},
  author = 	 {Krishnamurthy, A. and Kandasamy, A. and PÃ³czos, B. and Wasserman, L.},
  booktitle = 	 {ICML},
    year = 	 {2014},
}

@article{heroma2001techrep,
  title={Alpha divergence for classification, indexing and retrieval},
  author={Hero, A. O. and Ma, B. and Michel, O. and Gorman, J.},
  journal={Comm. and Sig. Proc. Lab. (CSPL), Dept. EECS, Univ. Michigan, Ann Arbor, Tech. Rep. 328},
  year={2001}
}

@article{heroma2002ieee,
  title={Applications of entropic spanning graphs},
  author={Hero, A. O. and Ma, B. and Michel, O. J. J. and Gorman, J.},
  journal={IEEE Signal Processing Magazine},
  year={2002}
}

@article{gretton2012kernel,
  title={A kernel two-sample test},
  author={Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={Mar},
  pages={723--773},
  year={2012}
}

@inproceedings{heusel2017gans,
  title={{GAN}s trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6626--6637},
  year={2017}
}

@InProceedings{hein05hilbertian,
  title = 	 {Hilbertian metrics and positive definite kernels on probability measures},
  author = 	 {Hein, M. and Bousquet, O.},
  booktitle = 	 {AISTATS},
    year = 	 {2005},
}

@InProceedings{poczos11alpha,
  title = 	 {On the estimation of alpha-divergences},
  author = 	 {Poczos, B. and Schneider, J.},
  booktitle = 	 {AISTATS},
    year = 	 {2011},
}

@InProceedings{singh14alpha,
  title = 	 {Generalized Exponential Concentration Inequality
for {R}{\'e}nyi Divergence Estimation},
  author = 	 {Singh, S. and Poczos, B.},
  booktitle = 	 {ICML},
    year = 	 {2014},
}

@article{wang09kl,
  title={Divergence estimation for multidimensional densities via k-nearest-neighbor distances},
  author={Wang, Q. and Kulkarni, S. R. and {Verd\'u}, S.},
  journal={IEEE Transactions on Information Theory},
  volume={55},
  number={5},
  year={2009},
  publisher={IEEE}
}

@InProceedings{perez08kl,
  title = 	 {Kullback-Leibler divergence estimation of continuous distributions},
  author = 	 {Perez-Cruz, F.},
  booktitle = 	 {IEEE International Symposium on Information Theory},
    year = 	 {2008},
}

@InProceedings{moon14followup,
  title = 	 {Multivariate f-divergence Estimation With Confidence},
  author = 	 {Moon, K. and Hero, A.},
  booktitle = 	 {NeurIPS},
    year = 	 {2014},
}

@article{kanamori12ratio,
  title={{f}-Divergence Estimation and Two-Sample Homogeneity Test under Semiparametric Density-Ratio Models},
  author={Kanamori, T. and Suzuki, T. and Sugiyama, M.},
  journal={IEEE Transactions on Information Theory},
  volume={58},
  number={2},
  year={2012},
  publisher={IEEE}
}


@inproceedings{liu2015faceattributes,
 author = {Ziwei Liu and Ping Luo and Xiaogang Wang and Xiaoou Tang},
 title = {Deep Learning Face Attributes in the Wild},
 booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
 month = {December},
 year = {2015} 
}

@book{pardo2005statistical,
  title={Statistical inference based on divergence measures},
  author={Pardo, Leandro},
  year={2005},
  publisher={Chapman and Hall/CRC}
}

@inproceedings{durrieu2012lower,
  title={Lower and upper bounds for approximation of the Kullback-Leibler divergence between Gaussian mixture models},
  author={Durrieu, J-L and Thiran, J-Ph and Kelly, Finnian},
  booktitle={2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4833--4836},
  year={2012},
  organization={Ieee}
}

@inproceedings{hershey2007approximating,
  title={Approximating the Kullback Leibler divergence between Gaussian mixture models},
  author={Hershey, John R and Olsen, Peder A},
  booktitle={2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP'07},
  volume={4},
  pages={IV--317},
  year={2007},
  organization={IEEE}
}