\section{Introduction and related literature}\label{sec:intro}

The estimation and minimization of divergences between probability distributions based on samples are fundamental problems of machine learning.
For example, maximum likelihood learning can be viewed as minimizing the Kullback-Leibler divergence $\KL(P_{\text{data}}\| P_{\text{model}})$ with respect to the model parameters. % where the model density is known.
More generally, generative modelling---most famously Variational Autoencoders and Generative Adversarial Networks \cite{kingma2013auto, goodfellow2014generative}---can be viewed as minimizing a divergence $\smash{D(P_{\text{data}}\| P_{\text{model}})}$ where $\smash{P_{\text{model}}}$ may be intractable.
In variational inference, an intractable posterior $p(z|x)$ is approximated with a tractable distribution $q(z)$ chosen to minimize $\smash{\KL\bigl(q(z) \| p(z|x)\bigr)}$.
The mutual information between two variables $\smash{I(X,Y)}$, core to information theory and Bayesian machine learning, is equivalent to $\smash{\KL(P_{X,Y} \| P_X P_Y)}$. 
Independence testing often involves estimating a divergence $\smash{D(P_{X,Y} \| P_X P_Y)}$, while two-sample testing (does $P=Q$?) involves estimating a divergence $D(P\|Q)$.
Additionally, one approach to domain adaptation, in which a classifier is learned on a distribution $P$ but tested on a distinct distribution $Q$, involves learning a feature map $\phi$ such that a divergence $\smash{D\left( \phi_\# P \| \phi_\# Q \right)}$ is minimized, where $\smash{\phi_\#}$ represents the push-forward operation \cite{ben2007analysis,ganin2016domain}.

In this work we consider the well-known family of $f$-divergences \cite{csiszar2004information, liese2006divergences} that includes amongst others the $\KL$, Jensen-Shannon ($\JS$), $\chi^2$, and $\alpha$-divergences as well as the Total Variation ($\TV$) and squared Hellinger ($\Hsq$) distances, the latter two of which play an important role in the statistics literature \cite{tsybakov2009}.
A significant body of work exists studying the estimation of the $f$-divergence $D_f(Q \| P)$ between general probability distributions $Q$ and $P$.
While the majority of this focuses on $\alpha$-divergences and closely related R\'enyi-$\alpha$ divergences \citep{poczos11alpha, singh14alpha, krishnamurthy14icml},
many works address specifically the KL-divergence \citep{perez08kl, wang09kl}
with fewer considering $f$-divergences in full generality \cite{nguyen10ratio, kanamori12ratio, moon14ensemble, moon14followup}.
Although the $\KL$-divergence is the most frequently encountered $f$-divergence in the machine learning literature, in recent years there has been a growing interest in other $f$-divergences \cite{nowozin2016f}, 
in particular in the variational inference community where they have been employed to derive alternative evidence lower bounds \cite{pmlr-v80-chen18k, li2016renyi, dieng2017variational}.


The main challenge in computing $D_f(Q \| P)$ is that it requires knowledge of either the densities of both $Q$ and $P$, or the density ratio $dQ/dP$.
In studying this problem, assumptions of differing strength can be made about $P$ and $Q$. 
In the weakest \emph{agnostic} setting, we may be given only a finite number of i.i.d\:samples from the distributions without any further knowledge about their densities.
As an example of stronger assumptions,
both distributions may be mixtures of Gaussians \cite{hershey2007approximating, durrieu2012lower}, 
or we may have access to samples from $Q$ and have full knowledge of $P$ \citep{heroma2001techrep, heroma2002ieee} as in e.g.\:model fitting.

Most of the literature on $f$-divergence estimation considers the weaker agnostic setting.
The lack of assumptions makes such work widely applicable, but comes at the cost of needing to work around estimation of either the densities of $P$ and $Q$ \cite{singh14alpha, krishnamurthy14icml} or the density ratio $dQ/dP$ \citep{nguyen10ratio, kanamori12ratio} from samples.
Both of these estimation problems are provably hard \citep{tsybakov2009, nguyen10ratio} and suffer rates---the speed at which the error of an estimator decays as a function of the number of samples $N$---of order $\smash{N^{-1/d}}$ when $P$ and $Q$ are defined over $\R^d$ unless their densities are sufficiently smooth.
This is a manifestation of the \emph{curse of dimensionality} and rates of this type are often called \emph{nonparametric}.
One could hope to estimate $D_f(P\|Q)$ without explicitly estimating the densities or their ratio and thus avoid suffering nonparametric rates, however a lower bound of the same order $\smash{N^{-1/d}}$ was recently proved for $\alpha$-divergences \citep{krishnamurthy14icml}, a sub-family of $f$-divergences.
While some works considering the agnostic setting provide rates for the bias and variance of the proposed estimator \cite{nguyen10ratio, krishnamurthy14icml} or even exponential tail bounds \citep{singh14alpha},
it is more common to only show that the estimators are asymptotically unbiased or consistent without proving specific rates of convergence \cite{wang09kl, poczos11alpha, kanamori12ratio}.


Motivated by recent advances in machine learning, we study a setting in which much stronger structural assumptions are made about the distributions.
Let $\X$ and $\Z$ be two finite dimensional Euclidean spaces.
We estimate the divergence $D_f(Q_Z\| P_Z)$ between two probability distributions $P_Z$ and $Q_Z$, both defined over $\Z$.
$P_Z$ has known density $p(z)$, while $Q_Z$ with density  $q(z)$ admits the factorization $\smash{q(z) := \int_\X q(z|x)q(x) dx}$ where access to independent samples from the distribution $Q_X$ with unknown density $q(x)$ and full knowledge of the conditional distribution $\smash{Q_{Z|X}}$ with density $q(z|x)$ are assumed.
In most cases $Q_Z$ is intractable due to the integral and so is $D_f(Q_Z \| P_Z)$.
As a concrete example, these assumptions are often satisfied in applications of modern unsupervised generative modeling with deep autoencoder architectures,
where $\X$ and $\Z$ would be \emph{data} and \emph{latent} spaces, $\smash{P_Z}$ the \emph{prior}, $\smash{Q_X}$ the \emph{data distribution}, $\smash{Q_{Z|X}}$ the \emph{encoder}, and $\smash{Q_Z}$ the \emph{aggregate posterior}.

Given independent observations $\smash{X_1, \ldots, X_N}$ from $\smash{Q_X}$, the finite mixture $\smash{\hat{Q}_Z^N := \frac{1}{N} \sum_{i=1}^N Q_{Z|X_i}}$ can be used to approximate the continuous mixture $\smash{Q_Z}$. 
{\bf Our main contribution} is to approximate the intractable $\smash{D_f(Q_Z \| P_Z)}$ with $\smash{D_f(\hat{Q}_Z^N \| P_Z)}$, a quantity that can be estimated to arbitrary precision using Monte-Carlo sampling since both distributions have known densities, and to theoretically study conditions under which this approximation is reasonable.
We call $\smash{D_f(\hat{Q}_Z^N \| P_Z)}$ the Random Mixture (RAM) estimator and derive rates at which it converges to $\smash{D_f(Q_Z \| P_Z)}$ as $N$ grows.
We also provide similar guarantees for RAM-MC---a practical Monte-Carlo based version of RAM.
By side-stepping the need to perform density estimation, we obtain \emph{parametric} rates of order $N^{-\gamma}$, where $\gamma$ is independent of the dimension (see Tables \ref{table:convergence} and \ref{table:concentration}), although the constants may still in general show exponential dependence on dimension.
This is in contrast to the agnostic setting where \emph{both} nonparametric rates and constants are exponential in dimension. 

Our results have immediate implications to existing literature.
For the particular case of the $\KL$ divergence, a similar approach has been \emph{heuristically} applied independently by several authors for estimating the mutual information \cite{poolevariational} and total correlation \cite{chen2018isolating}.
Our results provide strong theoretical grounding for these existing methods by showing sufficient conditions for their consistency.

A final piece of related work is \cite{burda2015importance}, which proposes to reduce the gap introduced by Jensen's inequality in the derivation of the classical evidence lower bound (ELBO) by using multiple Monte-Carlo samples from the approximate posterior $\smash{Q_{Z|X}}$.
This is similar in flavour to our approach, but fundamentally different since we use multiple samples from the \emph{data distribution} to reduce a different Jensen gap.
To avoid confusion, we note that replacing the ``regularizer'' term $\mathbb{E}_X[\KL(Q_{Z|X} \| P_Z)]$ of the classical ELBO with expectation of our estimator $\E_{\XN}[\KL(\hat{Q}_Z^N\| P_Z)]$ results in an upper bound of the classical ELBO (see Proposition~\ref{prop:upper-bound}) but is itself not in general an evidence lower bound:
{\addtolength{\abovedisplayskip}{-0.0mm}
\addtolength{\belowdisplayskip}{-3.0mm}
\begin{align*}
    \mathbb{E}_X \Big[ \mathbb{E}_{Q_{Z|X}} \log p(X|Z) - \KL(Q_{Z|X} \| P_Z ) \Big] \leq \mathbb{E}_X \Big[ \mathbb{E}_{Q_{Z|X}} \log p(X|Z) \Big] - \mathbb{E}_{\XN} \Big[ \KL(\hat{Q}_Z^N \| P_Z ) \Big].
\end{align*}}

The remainder of the paper is structured as follows.
In Section \ref{sec:theory} we introduce the RAM and RAM-MC estimators and present our main theoretical results, including rates of convergence for the bias (Theorems~\ref{thm:fast-KL-rate} and \ref{thm:convergence-rate-general}) and tail bounds (Theorems \ref{thm:concentration} and \ref{thm:mc-variance}).
In Section \ref{sec:experiments} we validate our results in both synthetic and real-data experiments. 
In Section \ref{sec:applications} we discuss further applications of our results.
We conclude in Section \ref{sec:conclusion}.

